{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"j2A7EsDgUZWE","executionInfo":{"status":"ok","timestamp":1655856132891,"user_tz":240,"elapsed":625,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}}},"outputs":[],"source":["import cv2\n","import os\n","import pickle\n","import numpy as np\n","import PIL as pil\n","from tqdm.auto import tqdm"]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQjqoOMgV4TI","outputId":"075b766c-a554-4f53-9ebb-7beb4673b4a0","executionInfo":{"status":"ok","timestamp":1655856159904,"user_tz":240,"elapsed":21080,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Drive files\n","\n","1. Aceptar invitación a la unidad compartida Patrones2022\n","\n","2. Abrir la unidad, y sobre la carpeta `home`, hacer click derecho y luego \"Añadir acceso directo a Drive\".\n","\n","Si eso funcionó, al ejecutar la siguiente celda, debería verse `parameters.json   pickled_features  sample_data\n","pickled_database  raw_database\t    trained_models` en el output."],"metadata":{"id":"culpx2NQWV_H"}},{"cell_type":"code","source":["HOME = \"/content/drive/My Drive/home/\"\n","!ls \"{HOME}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Sq2Th8bWRgp","outputId":"706431ae-989c-43e7-e0ad-1a05c5858995","executionInfo":{"status":"ok","timestamp":1655856163971,"user_tz":240,"elapsed":923,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["parameters.json   pickled_features  sample_data\n","pickled_database  raw_database\t    trained_models\n"]}]},{"cell_type":"code","source":["!ls \"{HOME}/raw_database\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cki2PHBWyK_k","outputId":"814a026c-9b74-4fc7-9e20-2b735af8162f","executionInfo":{"status":"ok","timestamp":1655856165515,"user_tz":240,"elapsed":241,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Bicicletas  Cachipun  Espinas  Letras  Lunares\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"U5ZXIY4JUZWL","executionInfo":{"status":"ok","timestamp":1655856695650,"user_tz":240,"elapsed":226,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}}},"outputs":[],"source":["def LoadImage(path: str, cmap: str = 'gray', echo: bool = True) -> np.ndarray:\n","    \"\"\"\n","    Load an image from a path\n","    \"\"\"\n","    if cmap == 'gray':\n","      cflag = cv2.IMREAD_GRAYSCALE\n","    elif cmap == 'rgb':\n","      cflag = cv2.IMREAD_COLOR\n","    else:\n","      print(f\"{cmap} is not a valid option\")\n","      raise AttributeError\n","    if echo:\n","      print(\"Image: \" + path)\n","    img = cv2.imread(path, cflag)\n","    if echo:\n","      print(\"Image size:\", img.shape)\n","    return img\n","\n","\n","def NofClasses(path: str) -> int:\n","    \"\"\"\n","    Get the classes in a directory\n","    \"\"\"\n","    return len(os.listdir(path)) - 1\n","\n","\n","def NofSamples(path: str) -> list:\n","    \"\"\"\n","    Get the number of samples for each class\n","    \"\"\"\n","    samples = []\n","    for subdir in os.listdir(path):\n","        samples.append(len(os.listdir(path + subdir)))\n","    return samples\n","\n","\n","def GetMinDim(path: str) -> tuple:\n","  \"\"\"\n","  Returns the smallest dimensions from every image in path.\n","  Path must be a nonempty folder, with at least one folder with images.\n","  \"\"\"\n","  minh = None\n","  minw = None\n","  for dir in os.listdir(path):\n","    for fil in os.listdir(path+dir):\n","      h, w = LoadImage(path + dir + '/' + fil, cmap=cmap, echo=echo).shape\n","      if not minh or h < minh:\n","        minh = h\n","      if not minw or w < minw:\n","        minw = w\n","      break\n","  return (minw, minh)\n","\n","\n","def BuildDataset(path: str, cmap: str = 'gray', echo: bool = False) -> tuple:\n","    \"\"\"\n","    Build a dataset from a directory, returns a tuple (X, y, #clas, [#sam])\n","    \"\"\"\n","    imdim = GetMinDim(path)\n","    if echo:\n","      print(f\"Smallest image size: {imdim}\")\n","    classes = NofClasses(path)\n","    samples = NofSamples(path)\n","    Xsam = np.zeros((sum(samples), imdim[1], imdim[0]))\n","    Ysam = np.zeros((sum(samples), 1))\n","    i = 0\n","    ii = 0\n","    echo = True\n","    for dir in os.listdir(path):\n","        for fil in tqdm(os.listdir(path + dir)):\n","            if fil == '.DS_Store':\n","              continue\n","            img = LoadImage(path + dir + '/' + fil, cmap=cmap, echo=echo)\n","            img = cv2.resize(img, imdim, interpolation = cv2.INTER_AREA) \n","            Xsam[ii] = img\n","            echo = False\n","            Ysam[ii] = i\n","            ii += 1\n","        i += 1\n","    return (Xsam, Ysam, classes, samples)\n","    "]},{"cell_type":"code","source":["%%script echo skipping\n","\n","# datasets = [\"Bicicletas\", \"Cachipun\", \"Espinas\", \"Letras\", \"Lunares\"]\n","\n","# for DSNAME in datasets:\n","#   out = BuildDataset(f\"{HOME}raw_database/{DSNAME}/\", echo=True)\n","#   Xsam, Ysam, clas, sam = out\n","\n","#   pick_insert = open(f'{HOME}pickled_database/{DSNAME}/Xsam.pkl', 'wb')\n","#   pickle.dump(Xsam, pick_insert)\n","#   pick_insert.close()\n","\n","#   pick_insert = open(f'{HOME}pickled_database/{DSNAME}/Ysam.pkl', 'wb')\n","#   pickle.dump(Ysam, pick_insert)\n","#   pick_insert.close()\n","\n","#   pick_insert = open(f'{HOME}pickled_database/{DSNAME}/clas.pkl', 'wb')\n","#   pickle.dump(clas, pick_insert)\n","#   pick_insert.close()\n","\n","#   pick_insert = open(f'{HOME}pickled_database/{DSNAME}/sam.pkl', 'wb')\n","#   pickle.dump(sam, pick_insert)X_raw[0]\n","#   pick_insert.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dss93f4xx_nG","outputId":"93503c6e-f39b-49ad-d187-e0af84fcd51d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["skipping\n"]}]},{"cell_type":"markdown","source":["## Bloque 1: Lectura de parámetros\n","Librerías necesarias para este bloque:"],"metadata":{"id":"DjFNKNButjv0"}},{"cell_type":"code","source":["import json"],"metadata":{"id":"YdTt6o8Ttq5S","executionInfo":{"status":"ok","timestamp":1655856799455,"user_tz":240,"elapsed":243,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class Block1:\n","  def __init__(self, path):\n","    '''\n","    Parse params file\n","\n","    INPUT:\n","      path: path to params file\n","    '''\n","    self.path = path\n","    self.load_params()\n","  \n","  def load_params(self):\n","    with open(self.path, encoding = 'utf-8') as file:\n","      params = json.load(file)\n","\n","      self.dataset = params['dataset']\n","      self.features = params['features']\n","      self.transformations = params['transformations']\n","      self.classifiers = []\n","      for c in params['classifiers']:\n","        formatted_input = []\n","        for c_name, c_params in c.items():\n","          formatted_input.append(c_name)\n","          for k, v in c_params.items():\n","            param = \"{0}:{1}\".format(k,v)\n","            formatted_input.append(param)\n","        \n","        self.classifiers.append(\"-\".join(formatted_input))"],"metadata":{"id":"ASVOLiYZtr1J","executionInfo":{"status":"ok","timestamp":1655856801000,"user_tz":240,"elapsed":245,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Ejemplo de uso"],"metadata":{"id":"PT7Ak-5Ttud4"}},{"cell_type":"code","source":["block1 = Block1(f'{HOME}parameters.json')\n","print(block1.classifiers)\n","print(block1.features)\n","print(block1.transformations)\n","print(block1.dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07U1CtedtvjJ","executionInfo":{"status":"ok","timestamp":1655856828177,"user_tz":240,"elapsed":254,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}},"outputId":"eb4adfdc-4f86-4dd8-957d-604a72a8621a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['FOREST-max_depth:3-n_estimators:100', 'KNN-n_neighbors:5']\n","['HOG-5x5x9', 'LBP-8x8']\n","[['CLEAN', 'MINMAX', 'KBEST-50'], ['PCA-5'], ['MINMAX', 'SFS-8'], ['PCA-5', 'ICA-2'], ['CLEAN', 'MINMAX', 'KBEST-50', 'SFS-10', 'PCA-5', 'ICA-2']]\n","Espinas\n"]}]},{"cell_type":"code","source":["del block1 # Importante para no llenar la RAM mientras se prueba"],"metadata":{"id":"AxXML77styLi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bloque 3: Extracción de características.\n","Librerías necesarias para este bloque:"],"metadata":{"id":"7hIGi3eiFUr0"}},{"cell_type":"code","source":["from IPython.display import clear_output\n","!pip3 install scipy==1.2\n","!pip3 install pybalu==0.2.5\n","clear_output()"],"metadata":{"id":"LNBGqeltm6sZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pybalu.feature_extraction import hog_features, lbp_features"],"metadata":{"id":"W7z6etBgILKp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Formato del nombre para cada característica soportada:\n","  - HoG: \"HOG-NxMxB\"\n","  - LBP: \"LBP-NxM\"\n","\n","Formato de archivo de características guardado en Drive:\n","\n","\"DSNAME_feature1+feature2.pkl\"\n","\n","Ej:\n","\"Bicicletas_LBP-5x5+HOG-7x7x9.pkl\""],"metadata":{"id":"yXaj5rg6lfuZ"}},{"cell_type":"code","source":["class Block3:\n","  def __init__(self, DSNAME, features, load_database=True, load_features=True, save=True):\n","      '''\n","      Extracts the features indicated from the DSNAME dataset.\n","\n","      INPUT:\n","        DSNAME: name of the dataset stored in Drive.\n","        features: array of strings following feature name convention,\n","        load_database: True -> loads pickled images directly.\n","        load_features: True -> tries to load the features from Drive in case they already\n","          been extracted before. If file does not exist, the features are extracted.\n","        save:          True -> saves the extracted features to Drive.\n","\n","      OUTPUT:\n","        Feature matrix is stored in the variable self.X.\n","        Ground truth is stored in the variable self.Ysam\n","      '''\n","      self.DSNAME = DSNAME\n","      self.features = features\n","      self.features_names, self.features_parameters = self.parse_features()\n","      self.save = save\n","\n","      if load_database:\n","        self.Xsam, self.Ysam, self.n_class, self.n_samples = self.import_pickled_dataset()\n","      else:\n","        self.Xsam, self.Ysam, self.n_class, self.n_samples = BuildDataset(f\"{HOME}raw_database/{self.DSNAME}/\", echo=True)\n","      self.n_class += 1\n","\n","      self.identifier = self.DSNAME + \"_\" + \"+\".join(self.features)\n","      if load_features:\n","        self.X = self.load_features()\n","      else:\n","        self.X = self.extract_features()\n","  \n","  def parse_features(self):\n","    features_names = []\n","    features_parameters = dict()\n","\n","    for feature in self.features:\n","      name = feature.split(\"-\")[0]\n","      parameters = feature.split(\"-\")[1]\n","      if name == \"HOG\":\n","        parameters = [int(param) for param in parameters.split(\"x\")]\n","\n","      elif name == \"LBP\":\n","        parameters = [int(param) for param in parameters.split(\"x\")]\n","      features_names.append(name)\n","      features_parameters[name] = parameters\n","    \n","    return features_names, features_parameters\n","    \n","\n","  def import_pickled_dataset(self):\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/Xsam.pkl','rb')\n","    Xsam = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/Ysam.pkl','rb')\n","    Ysam = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/clas.pkl','rb')\n","    n_class = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/sam.pkl','rb')\n","    n_samples = pickle.load(file_read)\n","    file_read.close()\n","    return Xsam, Ysam, n_class, n_samples\n","\n","  def load_features(self):\n","    try:\n","      read_file = open(f'{HOME}pickled_features/{self.DSNAME}/{self.identifier}.pkl', 'rb')\n","      X = pickle.load(read_file)\n","      return X\n","    except:\n","      print(f'Could not load {self.identifier}.pkl')\n","      X = self.extract_features()\n","      return X\n","\n","  def extract_features(self):\n","    total_images = sum(self.n_samples)\n","    feature_matrices = dict() # Dictionary indexed by feature name that saves feature matrix of feature.\n","\n","    # Initialize feature matrices\n","    for feature in self.features_names:\n","      if feature == \"LBP\":\n","        parameters = self.features_parameters[feature]\n","        M = 59*parameters[0]*parameters[1]\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","\n","      elif feature == \"HOG\":\n","        parameters = self.features_parameters[feature]\n","        M = parameters[0]*parameters[1]*parameters[2]\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","    \n","    # Extract each feature from each image.\n","    t = 0\n","    for image in self.Xsam:\n","      for feature in self.features_names:\n","        if feature == \"LBP\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = lbp_features(image, hdiv=parameters[0], vdiv=parameters[1], mapping='nri_uniform')\n","\n","        elif feature == \"HOG\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = hog_features(image, v_windows=parameters[0], h_windows=parameters[1], n_bins=parameters[2])\n","      t+=1\n","    \n","    # Concatenate features\n","    X = np.concatenate(tuple([feature_matrices[feature] for feature in self.features_names]), axis=1)\n","\n","    if self.save:\n","      try:\n","        pick_insert = open(f'{HOME}pickled_features/{self.DSNAME}/{self.identifier}.pkl', 'wb')\n","        pickle.dump(X, pick_insert)\n","        pick_insert.close()\n","      except:\n","        print(f'Could not save {self.identifier}.pkl')\n","\n","    return X\n","\n","  def __str__(self):\n","    return self.identifier"],"metadata":{"id":"mg2U1ZGol_ZA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ejemplo de uso"],"metadata":{"id":"KnKiJy0pJ2zy"}},{"cell_type":"code","source":["block3 = Block3(\"Espinas\", [\"LBP-6x6\", \"HOG-7x7x9\"], load_database=True, load_features=False, save=True)"],"metadata":{"id":"C4YZjirdqKez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(block3)\n","print(block3.features_names, block3.features_parameters)\n","print(block3.Xsam.shape, block3.Ysam.shape, block3.n_class, block3.n_samples)\n","print(block3.X.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ks4QvpQZuiLR","executionInfo":{"status":"ok","timestamp":1655762843935,"user_tz":240,"elapsed":5,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}},"outputId":"8f62a795-64ef-4070-cb8d-2dee330ea559"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Espinas_LBP-5x5+HOG-7x7x9\n","['LBP', 'HOG'] {'LBP': [5, 5], 'HOG': [7, 7, 9]}\n","(640, 100, 100) (640, 1) 2 [320, 320]\n","(640, 1916)\n"]}]},{"cell_type":"code","source":["del block3 # Importante para no llenar la RAM mientras se prueba"],"metadata":{"id":"fqdxzmgu2iDe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bloque 4: selección y transformación de características.\n","- Split Train-Validation\n","- Aplicación secuencial de alguna selección/transformación\n","  - Clean\n","  - MinMax Scaling\n","  - SelectKBest\n","  - SFS\n","  - PCA\n","  - ICA"],"metadata":{"id":"N1flNF6VEERS"}},{"cell_type":"code","source":["from IPython.display import clear_output\n","!pip3 install scipy==1.2\n","!pip3 install pybalu==0.2.5\n","clear_output()"],"metadata":{"id":"Lnx6MMNcm4Th"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from pybalu.feature_selection import clean\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.feature_selection import SelectKBest, chi2\n","from pybalu.feature_selection import sfs\n","from sklearn.decomposition import PCA, FastICA"],"metadata":{"id":"NVNNFHwbKdeh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CleanInterface:\n","\n","  def __init__(self, X):\n","    self.model = clean(X)\n","\n","  def transform(self, X):\n","    return X[:, self.model]\n","\n","class SFSInterface:\n","\n","  def __init__(self, X, y, s):\n","    self.model = sfs(X, y, s, show=False)\n","\n","  def transform(self, X):\n","    return X[:, self.model]"],"metadata":{"id":"wPSGbmLbOndK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Block4:\n","\n","  def __init__(self, X, y, sequence, ratio=0.3):\n","\n","    self.sequence = sequence\n","\n","    self.models = []\n","\n","    self.Xtrain, self.Xval, self.ytrain, self.yval = train_test_split(np.array(X), y, test_size=ratio, random_state=42, stratify=y)\n","\n","    self.Xtrain = self.interative_fit()\n","    self.Xval = self.transform(self.Xval)\n","\n","  def interative_fit(self):\n","\n","    X = self.Xtrain\n","\n","    for seq in self.sequence:\n","\n","      name = seq.split('-')[0]\n","\n","      if name == 'CLEAN':\n","        model = CleanInterface(X)\n","      elif name == 'MINMAX':\n","        model = MinMaxScaler().fit(X)\n","      else: \n","        param = int(seq.split('-')[1])\n","        if name == 'KBEST':\n","          model = SelectKBest(chi2, k=param).fit(X, self.ytrain)\n","        elif name == 'SFS':\n","          model = SFSInterface(X, self.ytrain, param)\n","        elif name == 'PCA':\n","          model = PCA(n_components=param).fit(X)\n","        elif name == 'ICA':\n","          model = FastICA(n_components=param, random_state=0).fit(X, self.ytrain)\n","        else:\n","          model = None\n","          print(f'No existe el modelo {name}')\n","\n","      self.models.append(model)\n","      X = model.transform(X)\n","\n","    return X\n","  \n","  def transform(self, X):\n","\n","    for model in self.models:\n","      X = model.transform(X)\n","    return X"],"metadata":{"id":"qYdP2hn7GEeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DATOS DE PRUEBA\n","!gdown --id 1CA-l9_JjdjG_4kTuavKf8Wm27dt0jyqT\n","clear_output()\n","\n","f = open('data.p', \"rb\")\n","data = pickle.load(f)\n","X = data['train']\n","\n","y = np.array([0 if i < 7000 else 1 for i in range(0, 14000)])\n","\n","X.shape, len(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3N8G2DVGJiA","outputId":"50d51bd1-06bd-460c-88e9-357e3be28d41"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14000, 1844), 14000)"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["block = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-50', 'SFS-10', 'PCA-5', 'ICA-2'])\n","block.Xtrain.shape, block.Xval.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxZu_HFjM7rE","outputId":"128fb986-4f6b-4d8f-ff3f-f5d2638f3856"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(9800, 1844) (4200, 1844) (9800,) (4200,)\n"]},{"output_type":"execute_result","data":{"text/plain":["((9800, 2), (4200, 2))"]},"metadata":{},"execution_count":72}]},{"cell_type":"markdown","source":["## Bloque 5"],"metadata":{"id":"il1epoYouaXm"}},{"cell_type":"markdown","source":["<div align=\"center\">\n","<img src=\"https://i.imgur.com/jxpf9U9.png\"></img>\n","</div>"],"metadata":{"id":"sWcojGo9uzEL"}},{"cell_type":"markdown","source":["### Modo de uso\n","\n","**Block5 recibe dos elementos:**\n","\n","- **blocks**: lista de instancias de la clase `Block4`\n","- **sequence**: lista de strings\n","\n","Para sequence, un elemento de la lista puede verse así:\n","\n","`FOREST-max_depth:3-n_estimators:100`\n","\n","Es decir: `{CLASSIFIER_NAME}-{PARAM_1_NAME}:{PARAM_1_VALUE}-{PARAM_2_NAME}:{PARAM_2_VALUE}`\n","\n","Puede que un Clasificador no tenga parámetros (como `QDA`). En ese caso basta sólo con el nombre.\n","\n","El mapping de nombre --> clasificador se aprecia en `self.classifiers` de `Block5`. Ahí mismo, se aprecian los `kwargs`, es decir los parámetros que recibe el constructor de dicho clasificador.\n","\n","Kwargs son los parámetros permitidos a personalizar. Lo demás será default.\n","Hay casos donde estos parámetros son obligatorios, y otros donde no.\n","A modo de maximizar el accuracy y evitar errores, ojalá todos estén presentes (mayor personalización).\n","\n","Al momento de instanciar el bloque, inmediatamente se comienza el proceso de búsqueda y validación del mejor modelo.\n","Al terminar, en el atributo `best_classifier` queda guardado el mejor clasificador. Es decir, una tupla de:\n","\n","`final_accuracy: float, <block4>: Block4, clf_class: object, params: dict`"],"metadata":{"id":"9AWMQRQEu9m4"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n","\n","from sklearn.svm import SVC\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import cross_val_score"],"metadata":{"id":"yon3u8FHub7l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import List\n","\n","class Params:\n","    N_FOLDS = 10\n","\n","class Block5:\n","\n","  def __init__(self, blocks: List['Block4'], sequence: List[str]):\n","\n","    self.blocks = blocks\n","    self.sequence = sequence\n","\n","    # Kwargs son los parámetros permitidos a personalizar. Lo demás será default.\n","    # Hay casos donde estos parámetros son obligatorios, y otros donde no.\n","    # A modo de maximizar el accuracy y evitar errores, ojalá todos estén presentes (mayor personalización).\n","    self.classifiers = {\n","          \"KNN\": {\"clf\": KNeighborsClassifier, \"kwargs\": [\"n_neighbors\"]},\n","          \"DMIN\": {\"clf\": NearestCentroid, \"kwargs\": []},\n","          \"SVC\": {\"clf\": SVC, \"kwargs\": [\"kernel\", \"C\", \"gamma\"]},\n","          \"TREE\": {\"clf\": DecisionTreeClassifier, \"kwargs\": [\"max_depth\"]},\n","          \"FOREST\": {\"clf\": RandomForestClassifier, \"kwargs\": [\"max_depth\", \"n_estimators\"]},\n","          \"NBAYES\": {\"clf\": GaussianNB, \"kwargs\": []},\n","          \"QDA\": {\"clf\": QuadraticDiscriminantAnalysis, \"kwargs\": []},\n","    }\n","\n","    self.best_classifier = self.interative_classify()\n","\n","  def interative_classify(self):\n","\n","    N = {}\n","\n","    for block in self.blocks:\n","      for seq in self.sequence:\n","        \n","        # \"FOREST-max_depth:3-n_estimators:100\" --> [\"FOREST\", \"max_depth:3-n_estimators:100\"]\n","        splitted_seq = seq.split('-')\n","        # name = \"FOREST\"\n","        name = splitted_seq[0]\n","        # params = [[\"max_depth\", \"3\"], [\"n_estimators\", \"100\"]]\n","        params = [clf.split(\":\") for clf in splitted_seq[1:]] if len(splitted_seq) > 1 else []\n","\n","        clf_info = self.classifiers[name]\n","        # RandomForestClassifier\n","        clf_class = clf_info[\"clf\"]\n","\n","        if name == \"SVC\": # necesita tipos de datos especiales\n","          param_types = {\"kernel\": str, \"C\": float, \"gamma\": int}\n","          params = {param_name: param_types[param_name](param_value) for param_name, param_value in params}\n","        else: # en caso contrario, todo parámetro se trata como <int>\n","          params = {param_name: int(param_value) for param_name, param_value in params}\n","\n","        crossval_score = self.cross_validation(block.Xtrain, block.ytrain, clf_class, params)\n","        N[crossval_score] = (block, clf_class, params)\n","\n","    # se obtiene el clasificador con mayor score en cross_validation\n","    max_n = max(N.keys())\n","    block, clf_class, params = N[max_n]\n","\n","    final_accuracy = self.hold_out(block.Xtrain, block.ytrain,\n","                                   block.Xval, block.yval,\n","                                   clf_class, params)\n","    \n","    return (final_accuracy, block, clf_class, params)\n","\n","\n","  def hold_out(self, X_train, y_train, X_test, y_test, clf, params) -> float:\n","    \"\"\" Retorna accuracy score al entrenar el clasificador en Train y probarlo en Test\"\"\"\n","    clf = clf(**params)\n","    fitted_clf = clf.fit(X_train, y_train)\n","    y_pred = fitted_clf.predict(X_test)\n","\n","    return accuracy_score(y_test, y_pred)\n","\n","  def cross_validation(self, X_train, y_train, clf, params, n_folds=Params.N_FOLDS) -> float:\n","    \"\"\" Retorna score de validación cruzada\"\"\"\n","    cv_scores = cross_val_score(clf(**params), X_train, y_train, cv=n_folds)\n","    return cv_scores.mean()"],"metadata":{"id":"lRwfLiwRvIdU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ejemplo de uso"],"metadata":{"id":"nsMB2dhIvJuB"}},{"cell_type":"code","source":["# USADO PARA DATOS SAMPLE EN EJEMPLO\n","\n","import pickle\n","\n","\n","def dump_sample_block4():\n","  with open('block4_A.pkl', 'wb') as handle:\n","      pickle.dump(block_A, handle)\n","  \n","  with open('block4_B.pkl', 'wb') as handle:\n","    pickle.dump(block_B, handle)\n","\n","  with open('block4_C.pkl', 'wb') as handle:\n","    pickle.dump(block_C, handle)\n","\n","\n","def load_sample_block4():\n","  base_path: str = \"/content/drive/MyDrive/home/sample_data/\"\n","\n","  with open(base_path +'block4_A.pkl', 'rb') as handle:\n","    block_A = pickle.load(handle)\n","\n","  with open(base_path + 'block4_B.pkl', 'rb') as handle:\n","    block_B = pickle.load(handle)\n","\n","  with open(base_path + 'block4_C.pkl', 'rb') as handle:\n","    block_C = pickle.load(handle)\n","\n","  return (block_A, block_B, block_C)"],"metadata":{"id":"EEDpLOPovKuh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# EJEMPLO CON DATOS EN DRIVE\n","\n","#block_A = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-50', 'SFS-10', 'PCA-5', 'ICA-2'])\n","#block_B = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-20', 'SFS-10', 'PCA-5'])\n","#block_C = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-20', 'ICA-2'])\n","\n","block_A, block_B, block_C = load_sample_block4()\n","BLOCKS = [block_A, block_B, block_C]\n","\n","results = Block5(BLOCKS, [\"FOREST-max_depth:3-n_estimators:100\", \"KNN-n_neighbors:15\",\n","                          \"NBAYES\", \"SVC-kernel:linear-C:0.025-gamma:2\"])\n","\n","results.best_classifier"],"metadata":{"id":"es7tLlCHvS6Y"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"6189f557f5bc4df41e92f7a00b3721572a07e07697aeafd1cb02e690e443f281"},"kernelspec":{"display_name":"Python 3.9.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"orig_nbformat":4,"colab":{"name":"base.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}