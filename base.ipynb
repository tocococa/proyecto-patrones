{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1656864992015,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"j2A7EsDgUZWE"},"outputs":[],"source":["import cv2\n","import os\n","import pickle\n","import numpy as np\n","import PIL as pil\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4063,"status":"ok","timestamp":1656864998265,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"BQjqoOMgV4TI","outputId":"724d34a6-1cb0-4830-f57c-c204cde7bfea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"culpx2NQWV_H"},"source":["## Drive files\n","\n","1. Aceptar invitación a la unidad compartida Patrones2022\n","\n","2. Abrir la unidad, y sobre la carpeta `home`, hacer click derecho y luego \"Añadir acceso directo a Drive\".\n","\n","Si eso funcionó, al ejecutar la siguiente celda, debería verse `parameters.json   pickled_features  sample_data\n","pickled_database  raw_database\t    trained_models` en el output."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1656865000638,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"1Sq2Th8bWRgp","outputId":"0b6c4e0d-5741-4b18-f628-27de07310fcb"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Diagrama.gslides   raw_database\t  sample_data\t  'video script.gdoc'\n"," parameters.json   'results (1).gsheet'   TODO.gdraw\n"," pickled_database   results.csv\t\t  trained_models\n"," pickled_features   results.gsheet\t  Video-1.mp4\n"]}],"source":["HOME = \"/content/drive/My Drive/home/\"\n","!ls \"{HOME}\""]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":367,"status":"ok","timestamp":1656865001883,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"Cki2PHBWyK_k","outputId":"868df5f3-3862-4d11-f4d0-7dfc69f0569e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Bicicletas  Cachipun  Espinas  Letras  Lunares\n"]}],"source":["!ls \"{HOME}/raw_database\""]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":301,"status":"ok","timestamp":1656865003128,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"U5ZXIY4JUZWL"},"outputs":[],"source":["def LoadImage(path: str, cmap: str = 'gray', echo: bool = True) -> np.ndarray:\n","    \"\"\"\n","    Load an image from a path\n","    \"\"\"\n","    if cmap == 'gray':\n","      cflag = cv2.IMREAD_GRAYSCALE\n","    elif cmap == 'rgb':\n","      cflag = cv2.IMREAD_COLOR\n","    else:\n","      print(f\"{cmap} is not a valid option\")\n","      raise AttributeError\n","    if echo:\n","      print(\"Image: \" + path)\n","    img = cv2.imread(path, cflag)\n","    if echo:\n","      print(\"Image size:\", img.shape)\n","    return img\n","\n","\n","def NofClasses(path: str) -> int:\n","    \"\"\"\n","    Get the classes in a directory\n","    \"\"\"\n","    return len(os.listdir(path)) - 1\n","\n","\n","def NofSamples(path: str) -> list:\n","    \"\"\"\n","    Get the number of samples for each class\n","    \"\"\"\n","    samples = []\n","    for subdir in os.listdir(path):\n","        samples.append(len(os.listdir(path + subdir)))\n","    return samples\n","\n","\n","def GetMinDim(path: str) -> tuple:\n","  \"\"\"\n","  Returns the smallest dimensions from every image in path.\n","  Path must be a nonempty folder, with at least one folder with images.\n","  \"\"\"\n","  minh = None\n","  minw = None\n","  for dir in os.listdir(path):\n","    for fil in os.listdir(path+dir):\n","      h, w = LoadImage(path + dir + '/' + fil).shape\n","      if not minh or h < minh:\n","        minh = h\n","      if not minw or w < minw:\n","        minw = w\n","      break\n","  return (minw, minh)\n","\n","\n","def BuildDataset(path: str, cmap: str = 'gray', echo: bool = False, max_samples: int=1800, min_factor: int=1) -> tuple:\n","    \"\"\"\n","    Build a dataset from a directory, returns a tuple (X, y, #clas, [#sam])\n","    \"\"\"\n","    imdim = GetMinDim(path)\n","    imdim = tuple([int(min_factor*x) for x in imdim])\n","    if echo:\n","      print(f\"Smallest image size: {imdim} with factor of {min_factor}\")\n","    classes = NofClasses(path)\n","    samples = NofSamples(path)\n","    Xsam = np.zeros((sum(samples), imdim[1], imdim[0]))\n","    Ysam = np.zeros((sum(samples), 1), \"int\")\n","    i = 0\n","    ii = 0\n","    echo = True\n","    for dir in os.listdir(path):\n","        c = 0\n","        for fil in tqdm(os.listdir(path + dir)):\n","            if c > max_samples:\n","              break\n","            if fil == '.DS_Store':\n","              continue\n","            img = LoadImage(path + dir + '/' + fil, cmap=cmap, echo=echo)\n","            img = cv2.resize(img, imdim, interpolation = cv2.INTER_AREA) \n","            Xsam[ii] = img\n","            echo = False\n","            Ysam[ii] = i\n","            ii += 1\n","            c += 1\n","        i += 1\n","    return (Xsam, Ysam, classes, samples)\n","    "]},{"cell_type":"markdown","metadata":{"id":"KUW6Ar2d0w63"},"source":["## Data Augmentation\n","\n","Realiza rotaciones de las imagenes en 15°, en sentido positivo  y negativo.\n","Se detiene cuando cumple el requisito de cantidad de imagenes.\n","Está configurado para lograr una cantidad de imagenes igual a: `max(#samples por clase) * 1.5`.\n","Es decir, si tenemos un dataset de:\n","```\n","Lunares\t\n","1: BCC   300  ---> 1800\n","2: BKL   600  ---> 1800\n","3: DF     80  ---> 1800\n","4: MEL   600  ---> 1800\n","5: NV   1200  ---> 1800 (el máximo * 50%)\n","6: VASC  110  ---> 1800\n","7: AKIEC 240  ---> 1800 \n","```"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":618,"status":"ok","timestamp":1656865009336,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"MzsWuPdQe6sg"},"outputs":[],"source":["from skimage import transform\n","import shutil\n","\n","\n","class DataAugmentator:\n","  def __init__(self, base_path: str):\n","    self.base_path = base_path\n","    self.subclasses_folders: list[str] = os.listdir(base_path)\n","    self.augment()\n","\n","  def create_rotator(self, angle):\n","    def rotate(img):\n","      return transform.rotate(img, angle=angle)\n","\n","    return rotate\n","  \n","  def augment(self):\n","    img_count = {}\n","\n","    for subfolder in self.subclasses_folders:\n","      img_count[subfolder] = len(os.listdir(self.base_path + subfolder))\n","\n","    target_count = max(img_count.values()) * 3 // 2\n","\n","    strategies = [self.create_rotator(angle) for angle in range(0, 180, 15)]\n","    strategies += [self.create_rotator(-angle) for angle in range(0, 180, 15)]\n","    strategy_generator = self.strategy_generator(strategies)\n","    \n","    for subfolder in self.subclasses_folders:\n","      print(subfolder)\n","\n","      class_folder_path = self.base_path + subfolder + \"/\"\n","      images = os.listdir(class_folder_path)\n","\n","      n_image = img_count[subfolder]\n","      img_pos = 0\n","\n","      if os.path.isdir(subfolder):\n","        shutil.rmtree(subfolder)\n","      os.mkdir(subfolder)\n","\n","      pbar = tqdm(total=target_count - len(images))\n","\n","      while n_image < target_count:\n","        img = LoadImage(class_folder_path + images[img_pos], echo=False)\n","        strat = next(strategy_generator)\n","\n","        with open(f\"{subfolder}/{n_image}.npy\", 'wb') as f:\n","            np.save(f, strat(img))\n","\n","        n_image += 1\n","        pbar.update(1)\n","\n","        if n_image % len(strategies) == 0:\n","          img_pos += 1\n","\n","      pbar.close()\n","    \n","  def strategy_generator(self, strategies):\n","    count = 0\n","    while True:\n","      count += 1\n","\n","      if count == len(strategies):\n","        count = 0\n","    \n","      yield strategies[count]\n","\n","\n","# data_augmentator = DataAugmentator(HOME + \"raw_database/Lunares/\")"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":242,"status":"ok","timestamp":1656865011525,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"HMCZ9K3DlGqS"},"outputs":[],"source":["from PIL import Image as im"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1656865012264,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"ZJPQQVzHgBAy"},"outputs":[],"source":["def AugmentAndStore(path: str, datasets: list, root: str='.') -> None:\n","  def Store(store_path: str, root_path: str) -> None:\n","    for aug in tqdm(os.listdir(root_path)):\n","      arr = np.load(f\"{root_path}/{aug}\")\n","      aug, *_ = aug.split('.')\n","      data = im.fromarray(np.uint8(arr * 255), 'L')\n","      data.save(f'{store_path}/{aug}.png')\n","\n","  def GetClasses(path: str) -> list:\n","    out = []\n","    for ds in os.listdir(path):\n","      out += os.listdir(f'{path}/{ds}')\n","    return out\n","\n","  cats = GetClasses(f\"{path}\")\n","  for ds in datasets:\n","    data_augmentator = DataAugmentator(f\"{path}/{ds}/\")\n","    ds_cats = os.listdir(f'{path}/{ds}')\n","    for cat in ds_cats:\n","      if cat in cats:\n","        print(f\"Store {cat}\")\n","        Store(f\"{path}/{ds}/{cat}\", f\"{root}/{cat}\")\n","        "]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1656864551434,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"inSGsP1xmDkO","outputId":"604ac9dc-c942-4592-e1fe-2451c32ae6dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["skipping\n"]}],"source":["%%script echo skipping\n","AugmentAndStore(f\"{HOME}raw_database\", [\"Bicicletas\", \"Cachipun\", \"Espinas\", \"Letras\", \"Lunares\"])"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1656864551434,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"dss93f4xx_nG","outputId":"c224dc9d-bc6f-4053-a149-1d4df8198be7"},"outputs":[{"name":"stdout","output_type":"stream","text":["skipping\n"]}],"source":["%%script echo skipping\n","datasets = [\"Lunares\"]\n","\n","for DSNAME in datasets:\n","  out = BuildDataset(f\"{HOME}raw_database/{DSNAME}/\", echo=True, min_factor=0.5)\n","  Xsam, Ysam, clas, sam = out\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/Ysam.pkl', 'wb')\n","  pickle.dump(Ysam, pick_insert)\n","  pick_insert.close()\n","  del Ysam\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/clas.pkl', 'wb')\n","  pickle.dump(clas, pick_insert)\n","  pick_insert.close()\n","  del clas\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/sam.pkl', 'wb')\n","  pickle.dump(sam, pick_insert)\n","  pick_insert.close()\n","  del sam\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/Xsam.pkl', 'wb')\n","  pickle.dump(Xsam, pick_insert)\n","  pick_insert.close()\n","  del Xsam\n","  "]},{"cell_type":"markdown","metadata":{"id":"DjFNKNButjv0"},"source":["## Bloque 2: Lectura de parámetros\n","Librerías necesarias para este bloque:"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1656865016154,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"YdTt6o8Ttq5S"},"outputs":[],"source":["import json"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1656865016599,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"ASVOLiYZtr1J"},"outputs":[],"source":["class Block2:\n","  def __init__(self, path):\n","    '''\n","    Parse params file\n","\n","    INPUT:\n","      path: path to params file\n","    '''\n","    self.path = path\n","    self.load_params()\n","  \n","  def load_params(self):\n","    with open(self.path, encoding = 'utf-8') as file:\n","      params = json.load(file)\n","\n","      self.dataset = params['dataset']\n","      self.features = params['features']\n","      self.transformations = params['transformations']\n","      self.classifiers = []\n","      for c in params['classifiers']:\n","        formatted_input = []\n","        for c_name, c_params in c.items():\n","          formatted_input.append(c_name)\n","          for k, v in c_params.items():\n","            param = \"{0}:{1}\".format(k,v)\n","            formatted_input.append(param)\n","        \n","        self.classifiers.append(\"-\".join(formatted_input))"]},{"cell_type":"markdown","metadata":{"id":"PT7Ak-5Ttud4"},"source":["Ejemplo de uso"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1656864551436,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"07U1CtedtvjJ","outputId":"02406d08-e98f-4c1f-cd36-64c0bd0ba9d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["skipping\n"]}],"source":["%%script echo skipping\n","block2 = Block2(f'{HOME}parameters.json')\n","print(block2.classifiers)\n","print(block2.features)\n","print(block2.transformations)\n","print(block2.dataset)\n","del block2 # Importante para no llenar la RAM mientras se prueba"]},{"cell_type":"markdown","metadata":{"id":"7hIGi3eiFUr0"},"source":["## Bloque 3: Extracción de características.\n","Librerías necesarias para este bloque:"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6007,"status":"ok","timestamp":1656865025776,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"LNBGqeltm6sZ"},"outputs":[],"source":["from IPython.display import clear_output\n","!pip3 install scipy==1.1.0\n","!pip3 install pybalu==0.2.5\n","clear_output()"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":248,"status":"ok","timestamp":1656867283891,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"W7z6etBgILKp"},"outputs":[],"source":["from pybalu.feature_extraction import hog_features, lbp_features, haralick_features, gabor_features, basic_int_features"]},{"cell_type":"markdown","metadata":{"id":"yXaj5rg6lfuZ"},"source":["Formato del nombre para cada característica soportada:\n","  - HoG: \"HOG-NxMxB\"\n","  - LBP: \"LBP-NxM\"\n","  - Haralick: \"HAR-N\"\n","  - Gabor: \"GAB-RxD\"\n","  - Basic intensity: \"BASIC-\"\n","\n","Formato de archivo de características guardado en Drive:\n","\n","\"DSNAME_feature1+feature2.pkl\"\n","\n","Ej:\n","\"Bicicletas_LBP-5x5+HOG-7x7x9.pkl\""]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":607,"status":"ok","timestamp":1656867429431,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"mg2U1ZGol_ZA"},"outputs":[],"source":["class Block3:\n","  def __init__(self, DSNAME, features, load_database=True, load_features=True, save=True):\n","      '''\n","      Extracts the features indicated from the DSNAME dataset.\n","\n","      INPUT:\n","        DSNAME: name of the dataset stored in Drive.\n","        features: array of strings following feature name convention,\n","        load_database: True -> loads pickled images directly.\n","        load_features: True -> tries to load the features from Drive in case they already\n","          been extracted before. If file does not exist, the features are extracted.\n","        save:          True -> saves the extracted features to Drive.\n","\n","      OUTPUT:\n","        Feature matrix is stored in the variable self.X.\n","        Ground truth is stored in the variable self.Ysam\n","      '''\n","      self.DSNAME = DSNAME\n","      self.features = features\n","      self.features_names, self.features_parameters = self.parse_features()\n","      self.save = save\n","\n","      if load_database:\n","        self.Xsam, self.Ysam, self.n_class, self.n_samples = self.import_pickled_dataset()\n","      else:\n","        self.Xsam, self.Ysam, self.n_class, self.n_samples = BuildDataset(f\"{HOME}raw_database/{self.DSNAME}/\", echo=True)\n","      self.n_class += 1\n","\n","      self.identifier = self.DSNAME + \"_\" + \"+\".join(self.features)\n","      if load_features:\n","        self.X = self.load_features()\n","      else:\n","        self.X = self.extract_features()\n","  \n","  def parse_features(self):\n","    features_names = []\n","    features_parameters = dict()\n","\n","    for feature in self.features:\n","      name = feature.split(\"-\")[0]\n","      parameters = feature.split(\"-\")[1]\n","      if name == \"HOG\":\n","        parameters = [int(param) for param in parameters.split(\"x\")]\n","\n","      elif name == \"LBP\":\n","        parameters = [int(param) for param in parameters.split(\"x\")]\n","\n","      elif name == \"HAR\":\n","        parameters = [int(parameters)]\n","      \n","      elif name == \"GAB\":\n","        parameters = [int(param) for param in parameters.split(\"x\")]\n","      \n","      elif name == \"BASIC\":\n","        parameters = []\n","\n","      features_names.append(name)\n","      features_parameters[name] = parameters\n","    \n","    return features_names, features_parameters\n","    \n","\n","  def import_pickled_dataset(self):\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/Xsam.pkl','rb')\n","    Xsam = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/Ysam.pkl','rb')\n","    Ysam = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/clas.pkl','rb')\n","    n_class = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/sam.pkl','rb')\n","    n_samples = pickle.load(file_read)\n","    file_read.close()\n","    return Xsam, Ysam, n_class, n_samples\n","\n","  def load_features(self):\n","    try:\n","      read_file = open(f'{HOME}pickled_features/{self.DSNAME}/{self.identifier}.pkl', 'rb')\n","      X = pickle.load(read_file)\n","      return X\n","    except:\n","      print(f'Could not load {self.identifier}.pkl')\n","      X = self.extract_features()\n","      return X\n","\n","  def extract_features(self):\n","    total_images = sum(self.n_samples)\n","    feature_matrices = dict() # Dictionary indexed by feature name that saves feature matrix of feature.\n","\n","    # Initialize feature matrices\n","    for feature in self.features_names:\n","      if feature == \"LBP\":\n","        parameters = self.features_parameters[feature]\n","        M = 59*parameters[0]*parameters[1]\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","\n","      elif feature == \"HOG\":\n","        parameters = self.features_parameters[feature]\n","        M = parameters[0]*parameters[1]*parameters[2]\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","      \n","      elif feature == \"HAR\":\n","        M = 28\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","    \n","      elif feature == \"GAB\":\n","        parameters = self.features_parameters[feature]\n","        M = parameters[0]*parameters[1] + 3\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","\n","      elif feature == \"BASIC\":\n","        M = 5\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","\n","    # Extract each feature from each image.\n","    t = 0\n","    for image in self.Xsam:\n","      for feature in self.features_names:\n","        if feature == \"LBP\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = lbp_features(image, hdiv=parameters[0], vdiv=parameters[1], mapping='nri_uniform')\n","\n","        elif feature == \"HOG\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = hog_features(image, v_windows=parameters[0], h_windows=parameters[1], n_bins=parameters[2])\n","\n","        elif feature == \"HAR\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = haralick_features(image, distance=parameters[0])\n","\n","        elif feature == \"GAB\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = gabor_features(image, rotations=parameters[0], dilations=parameters[1])\n","\n","        elif feature == \"BASIC\":\n","          feature_matrices[feature][t,:] = basic_int_features(image)[0:5]\n","\n","      t+=1\n","    \n","    # Concatenate features\n","    X = np.concatenate(tuple([feature_matrices[feature] for feature in self.features_names]), axis=1)\n","\n","    if self.save:\n","      try:\n","        pick_insert = open(f'{HOME}pickled_features/{self.DSNAME}/{self.identifier}.pkl', 'wb')\n","        pickle.dump(X, pick_insert)\n","        pick_insert.close()\n","      except:\n","        print(f'Could not save {self.identifier}.pkl')\n","\n","    return X\n","\n","  def __str__(self):\n","    return self.identifier"]},{"cell_type":"markdown","metadata":{"id":"KnKiJy0pJ2zy"},"source":["Ejemplo de uso"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1190569,"status":"ok","timestamp":1656868620895,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"},"user_tz":240},"id":"C4YZjirdqKez","outputId":"17abeffa-fe5c-4e4c-9a12-85ee55f224dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Lunares_LBP-3x3+HOG-3x3x9+HAR-1+GAB-2x2+BASIC-\n","['LBP', 'HOG', 'HAR', 'GAB', 'BASIC'] {'LBP': [3, 3], 'HOG': [3, 3, 9], 'HAR': [1], 'GAB': [2, 2], 'BASIC': []}\n","(12600, 128, 128) (12600, 1) 7 [1800, 1800, 1800, 1800, 1800, 1800, 1800]\n","(12600, 652)\n"]}],"source":["%%script echo skipping\n","block3 = Block3(\"Lunares\", [\"LBP-3x3\", \"HOG-3x3x9\", \"HAR-1\", \"GAB-2x2\", \"BASIC-\"], load_database=True, load_features=False, save=True)\n","\n","print(block3)\n","print(block3.features_names, block3.features_parameters)\n","print(block3.Xsam.shape, block3.Ysam.shape, block3.n_class, block3.n_samples)\n","print(block3.X.shape)\n","\n","del block3 # Importante para no llenar la RAM mientras se prueba"]},{"cell_type":"markdown","metadata":{"id":"N1flNF6VEERS"},"source":["## Bloque 4: selección y transformación de características.\n","- Split Train-Validation\n","- Aplicación secuencial de alguna selección/transformación\n","  - Clean\n","  - MinMax Scaling\n","  - SelectKBest\n","  - SFS\n","  - PCA\n","  - ICA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lnx6MMNcm4Th"},"outputs":[],"source":["from IPython.display import clear_output\n","!pip3 install scipy==1.1.0\n","!pip3 install pybalu==0.2.5\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVNNFHwbKdeh"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from pybalu.feature_selection import clean\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.feature_selection import SelectKBest, chi2\n","from pybalu.feature_selection import sfs\n","from sklearn.decomposition import PCA, FastICA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPSGbmLbOndK"},"outputs":[],"source":["class CleanInterface:\n","\n","  def __init__(self, X):\n","    self.model = clean(X)\n","\n","  def transform(self, X):\n","    return X[:, self.model]\n","\n","class SFSInterface:\n","\n","  def __init__(self, X, y, s):\n","    self.model = sfs(X, y, s, show=False)\n","\n","  def transform(self, X):\n","    return X[:, self.model]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYdP2hn7GEeo"},"outputs":[],"source":["class Block4:\n","\n","  def __init__(self, X, y, sequence, ratio=0.3):\n","\n","    self.sequence = sequence\n","\n","    self.models = []\n","\n","    self.Xtrain, self.Xval, self.ytrain, self.yval = train_test_split(np.array(X), y, test_size=ratio, random_state=42, stratify=y)\n","\n","    self.Xtrain = self.interative_fit()\n","    self.Xval = self.transform(self.Xval)\n","\n","  def interative_fit(self):\n","\n","    X = self.Xtrain\n","\n","    for seq in self.sequence:\n","\n","      name = seq.split('-')[0]\n","\n","      if name == 'CLEAN':\n","        model = CleanInterface(X)\n","      elif name == 'MINMAX':\n","        model = MinMaxScaler().fit(X)\n","      else: \n","        param = int(seq.split('-')[1])\n","        if name == 'KBEST':\n","          model = SelectKBest(chi2, k=param).fit(X, self.ytrain)\n","        elif name == 'SFS':\n","          model = SFSInterface(X, self.ytrain, param)\n","        elif name == 'PCA':\n","          model = PCA(n_components=param).fit(X)\n","        elif name == 'ICA':\n","          model = FastICA(n_components=param, random_state=0).fit(X, self.ytrain)\n","        else:\n","          model = None\n","          print(f'No existe el modelo {name}')\n","\n","      self.models.append(model)\n","      X = model.transform(X)\n","\n","    return X\n","  \n","  def transform(self, X):\n","\n","    for model in self.models:\n","      X = model.transform(X)\n","    return X"]},{"cell_type":"markdown","metadata":{"id":"qpFT1cz9yTAa"},"source":["Ejemplo de uso"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3N8G2DVGJiA","outputId":"017f8333-bb88-4045-984f-4c901bc03095"},"outputs":[{"name":"stdout","output_type":"stream","text":["skipping\n"]}],"source":["%%script echo skipping\n","!gdown --id 1CA-l9_JjdjG_4kTuavKf8Wm27dt0jyqT\n","clear_output()\n","f = open('data.p', \"rb\")\n","data = pickle.load(f)\n","X = data['train']\n","y = np.array([0 if i < 7000 else 1 for i in range(0, 14000)])\n","X.shape, len(y)\n","\n","block = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-50', 'SFS-10', 'PCA-5', 'ICA-2'])\n","block.Xtrain.shape, block.Xval.shape"]},{"cell_type":"markdown","metadata":{"id":"il1epoYouaXm"},"source":["## Bloque 5"]},{"cell_type":"markdown","metadata":{"id":"sWcojGo9uzEL"},"source":["<div align=\"center\">\n","<img src=\"https://i.imgur.com/jxpf9U9.png\"></img>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"9AWMQRQEu9m4"},"source":["### Modo de uso\n","\n","**Block5 recibe dos elementos:**\n","\n","- **blocks**: lista de instancias de la clase `Block4`\n","- **sequence**: lista de strings\n","\n","Para sequence, un elemento de la lista puede verse así:\n","\n","`FOREST-max_depth:3-n_estimators:100`\n","\n","Es decir: `{CLASSIFIER_NAME}-{PARAM_1_NAME}:{PARAM_1_VALUE}-{PARAM_2_NAME}:{PARAM_2_VALUE}`\n","\n","Puede que un Clasificador no tenga parámetros (como `QDA`). En ese caso basta sólo con el nombre.\n","\n","El mapping de nombre --> clasificador se aprecia en `self.classifiers` de `Block5`. Ahí mismo, se aprecian los `kwargs`, es decir los parámetros que recibe el constructor de dicho clasificador.\n","\n","Kwargs son los parámetros permitidos a personalizar. Lo demás será default.\n","Hay casos donde estos parámetros son obligatorios, y otros donde no.\n","A modo de maximizar el accuracy y evitar errores, ojalá todos estén presentes (mayor personalización).\n","\n","Al momento de instanciar el bloque, inmediatamente se comienza el proceso de búsqueda y validación del mejor modelo.\n","Al terminar, en el atributo `best_classifier` queda guardado el mejor clasificador. Es decir, una tupla de:\n","\n","`final_accuracy: float, <block4>: Block4, clf_class: object, params: dict`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yon3u8FHub7l"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n","\n","from sklearn.svm import SVC\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import cross_val_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRwfLiwRvIdU"},"outputs":[],"source":["from typing import List\n","\n","class Params:\n","    N_FOLDS = 10\n","\n","class Block5:\n","\n","  def __init__(self, blocks: List['Block4'], sequence: List[str]):\n","\n","    self.blocks = blocks\n","    self.sequence = sequence\n","\n","    # Kwargs son los parámetros permitidos a personalizar. Lo demás será default.\n","    # Hay casos donde estos parámetros son obligatorios, y otros donde no.\n","    # A modo de maximizar el accuracy y evitar errores, ojalá todos estén presentes (mayor personalización).\n","    self.classifiers = {\n","          \"KNN\": {\"clf\": KNeighborsClassifier, \"kwargs\": [\"n_neighbors\"]},\n","          \"DMIN\": {\"clf\": NearestCentroid, \"kwargs\": []},\n","          \"SVC\": {\"clf\": SVC, \"kwargs\": [\"kernel\", \"C\", \"gamma\"]},\n","          \"TREE\": {\"clf\": DecisionTreeClassifier, \"kwargs\": [\"max_depth\"]},\n","          \"FOREST\": {\"clf\": RandomForestClassifier, \"kwargs\": [\"max_depth\", \"n_estimators\"]},\n","          \"NBAYES\": {\"clf\": GaussianNB, \"kwargs\": []},\n","          \"QDA\": {\"clf\": QuadraticDiscriminantAnalysis, \"kwargs\": []},\n","    }\n","\n","    self.best_classifier = self.interative_classify()\n","\n","  def interative_classify(self):\n","\n","    N = {}\n","\n","    for block in self.blocks:\n","      for seq in self.sequence:\n","        \n","        # \"FOREST-max_depth:3-n_estimators:100\" --> [\"FOREST\", \"max_depth:3-n_estimators:100\"]\n","        splitted_seq = seq.split('-')\n","        # name = \"FOREST\"\n","        name = splitted_seq[0]\n","        # params = [[\"max_depth\", \"3\"], [\"n_estimators\", \"100\"]]\n","        params = [clf.split(\":\") for clf in splitted_seq[1:]] if len(splitted_seq) > 1 else []\n","\n","        clf_info = self.classifiers[name]\n","        # RandomForestClassifier\n","        clf_class = clf_info[\"clf\"]\n","\n","        if name == \"SVC\": # necesita tipos de datos especiales\n","          param_types = {\"kernel\": str, \"C\": float, \"gamma\": int}\n","          params = {param_name: param_types[param_name](param_value) for param_name, param_value in params}\n","        else: # en caso contrario, todo parámetro se trata como <int>\n","          params = {param_name: int(param_value) for param_name, param_value in params}\n","\n","        crossval_score = self.cross_validation(block.Xtrain, block.ytrain, clf_class, params)\n","        N[crossval_score] = (block, clf_class, params, seq)\n","\n","    # se obtiene el clasificador con mayor score en cross_validation\n","    max_n = max(N.keys())\n","    block, clf_class, params, seq = N[max_n]\n","\n","    final_accuracy = self.hold_out(block.Xtrain, block.ytrain,\n","                                   block.Xval, block.yval,\n","                                   clf_class, params)\n","    \n","    clf = clf_class(**params)\n","    clf.fit(block.Xtrain, block.ytrain)\n","\n","    return (final_accuracy, block, clf_class, params, seq, clf)\n","\n","\n","  def hold_out(self, X_train, y_train, X_test, y_test, clf, params) -> float:\n","    \"\"\" Retorna accuracy score al entrenar el clasificador en Train y probarlo en Test\"\"\"\n","    clf = clf(**params)\n","    fitted_clf = clf.fit(X_train, y_train)\n","    y_pred = fitted_clf.predict(X_test)\n","\n","    return accuracy_score(y_test, y_pred)\n","\n","  def cross_validation(self, X_train, y_train, clf, params, n_folds=Params.N_FOLDS) -> float:\n","    \"\"\" Retorna score de validación cruzada\"\"\"\n","    cv_scores = cross_val_score(clf(**params), X_train, y_train, cv=n_folds)\n","    return cv_scores.mean()"]},{"cell_type":"markdown","metadata":{"id":"nsMB2dhIvJuB"},"source":["Ejemplo de uso"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEDpLOPovKuh","outputId":"aee6019d-8a33-4776-86d7-beb1d5fdbc02"},"outputs":[{"name":"stdout","output_type":"stream","text":["skipping\n"]}],"source":["%%script echo skipping\n","import pickle\n","\n","\n","def dump_sample_block4():\n","  with open('block4_A.pkl', 'wb') as handle:\n","      pickle.dump(block_A, handle)\n","  \n","  with open('block4_B.pkl', 'wb') as handle:\n","    pickle.dump(block_B, handle)\n","\n","  with open('block4_C.pkl', 'wb') as handle:\n","    pickle.dump(block_C, handle)\n","\n","\n","def load_sample_block4():\n","  base_path: str = \"/content/drive/MyDrive/home/sample_data/\"\n","\n","  with open(base_path +'block4_A.pkl', 'rb') as handle:\n","    block_A = pickle.load(handle)\n","\n","  with open(base_path + 'block4_B.pkl', 'rb') as handle:\n","    block_B = pickle.load(handle)\n","\n","  with open(base_path + 'block4_C.pkl', 'rb') as handle:\n","    block_C = pickle.load(handle)\n","\n","  return (block_A, block_B, block_C)\n","\n","#block_A = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-50', 'SFS-10', 'PCA-5', 'ICA-2'])\n","#block_B = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-20', 'SFS-10', 'PCA-5'])\n","#block_C = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-20', 'ICA-2'])\n","\n","block_A, block_B, block_C = load_sample_block4()\n","BLOCKS = [block_A, block_B, block_C]\n","\n","results = Block5(BLOCKS, [\"FOREST-max_depth:3-n_estimators:100\", \"KNN-n_neighbors:15\",\n","                          \"NBAYES\", \"SVC-kernel:linear-C:0.025-gamma:2\"])\n","\n","results.best_classifier"]},{"cell_type":"markdown","metadata":{"id":"jPsPu_hBy9Hk"},"source":["# AutoPR"]},{"cell_type":"markdown","metadata":{"id":"Mk518Bhm7DyL"},"source":["Convención:\n","\n","\\[DATASET]\\_\\[feature1+feature2+...\\]\\_\\[transf1+transf2]\\_\\[classifier-param1name:param1value-param2name:param2value\\]\n","\n","Ej"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mZLnzQYzy8d0"},"outputs":[],"source":["class AutoPR:\n","  def __init__(self, path, load_database=True, load_features=True, save=False):\n","    self.parameters_path = path\n","    self.load_database = load_database\n","    self.load_features = load_features\n","    self.save = save\n","    # (CV_final_accuracy, block4, clf_class, params, seq, clf_instance)\n","    self.best_classifier_data, self.block2 = self.main()\n","    self.identifier = self.block2.dataset + \"_\" + \"+\".join(self.block2.features) + \"_\" + \"+\".join(self.best_classifier_data[1].sequence) + \"_\" + self.best_classifier_data[4]\n","\n","    if self.save:\n","      self.export_classifier()\n","  \n","  def main(self):\n","    block2 = Block2(self.parameters_path)\n","\n","    block3 = Block3(block2.dataset, block2.features, load_database=self.load_database, load_features=self.load_features, save=self.save)\n","\n","    block4_list = []\n","    for transformation in block2.transformations:\n","      block4 = Block4(block3.X, block3.Ysam, transformation)\n","      block4_list.append(block4)\n","    \n","    block5 = Block5(block4_list, block2.classifiers)\n","\n","    return block5.best_classifier, block2\n","  \n","  def predict(self, X, features, feature_names, feature_params):\n","    # Extraer características a partir de block1.features\n","    def extract_features(X, features, feature_names, feature_params):\n","      # esto es exactamente lo mismo que hay en Block3, pero no puedo \n","      # heredar/sobreescribirlo sin matar el BuildDataset porque tiene\n","      # hardocdeado el path para cargar un DS_raw y no el de demo\n","      total_images = X.shape[0]\n","      feature_matrices = dict() # Dictionary indexed by feature name that saves feature matrix of feature.\n","\n","      # Initialize feature matrices\n","      for feature in features:\n","        if feature == \"LBP\":\n","          parameters = feature_params[feature]\n","          M = 59*parameters[0]*parameters[1]\n","          Xfeat = np.zeros((total_images,M))\n","          feature_matrices[feature] = Xfeat\n","\n","        elif feature == \"HOG\":\n","          parameters = feature_params[feature]\n","          M = parameters[0]*parameters[1]*parameters[2]\n","          Xfeat = np.zeros((total_images,M))\n","          feature_matrices[feature] = Xfeat\n","        \n","        elif feature == \"HAR\":\n","          M = 28\n","          Xfeat = np.zeros((total_images,M))\n","          feature_matrices[feature] = Xfeat\n","      \n","        elif feature == \"GAB\":\n","          parameters = feature_params[feature]\n","          M = parameters[0]*parameters[1] + 3\n","          Xfeat = np.zeros((total_images,M))\n","          feature_matrices[feature] = Xfeat\n","\n","        elif feature == \"BASIC\":\n","          M = 5\n","          Xfeat = np.zeros((total_images,M))\n","          feature_matrices[feature] = Xfeat\n","\n","      # Extract each feature from each image.\n","      t = 0\n","      for image in self.Xsam:\n","        for feature in feature_names:\n","          if feature == \"LBP\":\n","            parameters = feature_params[feature]\n","            feature_matrices[feature][t,:] = lbp_features(image, hdiv=parameters[0], vdiv=parameters[1], mapping='nri_uniform')\n","\n","          elif feature == \"HOG\":\n","            parameters = feature_params[feature]\n","            feature_matrices[feature][t,:] = hog_features(image, v_windows=parameters[0], h_windows=parameters[1], n_bins=parameters[2])\n","\n","          elif feature == \"HAR\":\n","            parameters = feature_params[feature]\n","            feature_matrices[feature][t,:] = haralick_features(image, distance=parameters[0])\n","\n","          elif feature == \"GAB\":\n","            parameters = feature_params[feature]\n","            feature_matrices[feature][t,:] = gabor_features(image, rotations=parameters[0], dilations=parameters[1])\n","\n","          elif feature == \"BASIC\":\n","            feature_matrices[feature][t,:] = basic_int_features(image)[0:5]\n","\n","        t+=1\n","      \n","      # Concatenate features\n","      X = np.concatenate(tuple([feature_matrices[feature] for feature in feature_names]), axis=1)\n","\n","      return X\n","\n","    X = extract_features(X, features, feature_names, feature_params)\n","\n","    # Aplicar transfomaciones a la imagen\n","    # Aplicar self.best_classifier_data[1].transform()\n","    X = self.best_classifier_data[1].transform(X)\n","\n","    # Hacer self.best_classifier_data[4].predict()\n","    X = self.best_classifier_data[4].predict(X)\n","\n","    return X\n","\n","  def export_classifier(self):\n","      try:\n","        pick_insert = open(f'{HOME}trained_models/{self.block1.dataset}/{self.identifier}.pkl', 'wb')\n","        pickle.dump(self, pick_insert)\n","        pick_insert.close()\n","      except:\n","        print(f'Could not save {self.identifier}.pkl')\n","    "]},{"cell_type":"markdown","metadata":{"id":"hvnKwuz82Hc0"},"source":["Ejemplo de uso"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAxZyaWy2ITK"},"outputs":[],"source":["%%script echo skipping\n","autopr = AutoPR(f'{HOME}parameters.json', load_database=True, load_features=True, save=True)\n","autopr.best_classifier_data\n","autopr.identifier"]},{"cell_type":"markdown","metadata":{"id":"9YfvKb_pjkbz"},"source":["# Log AutoPR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lS8wL6dHnpRU"},"outputs":[],"source":["import csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGNRfTSFjpNJ"},"outputs":[],"source":["def autopr_data_to_csv(autopr: AutoPR, path: str=f'{HOME}results.csv', echo=False):\n","  with open(path, 'a', newline='') as fil:\n","    acc, *rest, clas = autopr.best_classifier_data\n","    row = [autopr.block1.dataset, autopr.block1.features, autopr.best_classifier_data[1].sequence, clas, acc, autopr.identifier]\n","    if echo:\n","      print(row)\n","    writer = csv.writer(fil, delimiter=',')\n","    writer.writerow(row)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBkcC438yJB8"},"outputs":[],"source":["%%script echo skipping\n","autopr_data_to_csv(autopr, echo=True)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"base.ipynb","provenance":[]},"interpreter":{"hash":"6189f557f5bc4df41e92f7a00b3721572a07e07697aeafd1cb02e690e443f281"},"kernelspec":{"display_name":"Python 3.9.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
