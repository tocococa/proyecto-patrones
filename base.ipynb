{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"j2A7EsDgUZWE","executionInfo":{"status":"ok","timestamp":1656985535405,"user_tz":240,"elapsed":380,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["import cv2\n","import os\n","import pickle\n","import numpy as np\n","import PIL as pil\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQjqoOMgV4TI","outputId":"bb894adb-8e89-4747-fb89-57e9e1c6b304","executionInfo":{"status":"ok","timestamp":1656985539718,"user_tz":240,"elapsed":3945,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"culpx2NQWV_H"},"source":["# Drive files (Bloque 1)\n","\n","1. Aceptar invitación a la unidad compartida Patrones2022\n","\n","2. Abrir la unidad, y sobre la carpeta `home`, hacer click derecho y luego \"Añadir acceso directo a Drive\".\n","\n","Si eso funcionó, al ejecutar la siguiente celda, debería verse `parameters.json   pickled_features  sample_data\n","pickled_database  raw_database\t    trained_models` en el output."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Sq2Th8bWRgp","outputId":"3fe1c893-fb45-4121-cdbf-032cbaa03395","executionInfo":{"status":"ok","timestamp":1656985539719,"user_tz":240,"elapsed":14,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":[" demo_data\t    pickled_features\t  results.gsheet   Video-1.mp4\n"," Diagrama.gslides   raw_database\t  sample_data\t  'video script.gdoc'\n"," parameters\t   'results (1).gsheet'   TODO.gdraw\n"," pickled_database   results.csv\t\t  trained_models\n"]}],"source":["HOME = \"/content/drive/My Drive/home/\"\n","!ls \"{HOME}\""]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cki2PHBWyK_k","outputId":"c588d811-9dd3-4b64-a9ff-1e28728147ff","executionInfo":{"status":"ok","timestamp":1656985540099,"user_tz":240,"elapsed":387,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Bicicletas  Cachipun  Espinas  Letras  Lunares\n"]}],"source":["!ls \"{HOME}/raw_database\""]},{"cell_type":"code","execution_count":61,"metadata":{"id":"U5ZXIY4JUZWL","executionInfo":{"status":"ok","timestamp":1656987249267,"user_tz":240,"elapsed":243,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["def LoadImage(path: str, cmap: str = 'gray', echo: bool = True) -> np.ndarray:\n","    \"\"\"\n","    Load an image from a path\n","    \"\"\"\n","    if cmap == 'gray':\n","      cflag = cv2.IMREAD_GRAYSCALE\n","    elif cmap == 'rgb':\n","      cflag = cv2.IMREAD_COLOR\n","    else:\n","      print(f\"{cmap} is not a valid option\")\n","      raise AttributeError\n","    if echo:\n","      print(\"Image: \" + path)\n","    img = cv2.imread(path, cflag)\n","    if echo:\n","      print(\"Image size:\", img.shape)\n","    return img\n","\n","\n","def NofClasses(path: str) -> int:\n","    \"\"\"\n","    Get the classes in a directory\n","    \"\"\"\n","    return len(os.listdir(path)) - 1\n","\n","\n","def NofSamples(path: str) -> list:\n","    \"\"\"\n","    Get the number of samples for each class\n","    \"\"\"\n","    samples = []\n","    for subdir in os.listdir(path):\n","        samples.append(len(os.listdir(path + subdir)))\n","    return samples\n","\n","\n","def GetMinDim(path: str) -> tuple:\n","  \"\"\"\n","  Returns the smallest dimensions from every image in path.\n","  Path must be a nonempty folder, with at least one folder with images.\n","  \"\"\"\n","  minh = None\n","  minw = None\n","  for dir in os.listdir(path):\n","    for fil in os.listdir(path+dir):\n","      h, w = LoadImage(path + dir + '/' + fil).shape\n","      if not minh or h < minh:\n","        minh = h\n","      if not minw or w < minw:\n","        minw = w\n","      break\n","  return (minw, minh)\n","\n","\n","def BuildDataset(path: str, cmap: str = 'gray', echo: bool = False, max_samples: int=1800, min_factor: int=1) -> tuple:\n","    \"\"\"\n","    Build a dataset from a directory, returns a tuple (X, y, #clas, [#sam])\n","    \"\"\"\n","    imdim = GetMinDim(path)\n","    imdim = tuple([int(min_factor*x) for x in imdim])\n","    if echo:\n","      print(f\"Smallest image size: {imdim} with factor of {min_factor}\")\n","    classes = NofClasses(path)\n","    samples = NofSamples(path)\n","    Xsam = np.zeros((sum(samples), imdim[1], imdim[0]))\n","    Ysam = np.zeros((sum(samples), 1), \"int\")\n","    i = 0\n","    ii = 0\n","    echo = True\n","    for dir in os.listdir(path):\n","        # la clase corresponde al numero que aparece antes del _\n","        # en el dataset\n","        y, *_ = dir.split('_')\n","        y = int(y)\n","        c = 0\n","        for fil in tqdm(os.listdir(path + dir)):\n","            if c > max_samples:\n","              break\n","            if fil == '.DS_Store':\n","              continue\n","            #img = LoadImage(path + dir + '/' + fil, cmap=cmap, echo=echo)\n","            #img = cv2.resize(img, imdim, interpolation = cv2.INTER_AREA) \n","            #Xsam[ii] = img\n","            echo = False\n","            Ysam[ii] = y\n","            ii += 1\n","            c += 1\n","        i += 1\n","    return (Xsam, Ysam, classes, samples, imdim)\n","    "]},{"cell_type":"markdown","metadata":{"id":"KUW6Ar2d0w63"},"source":["## Data Augmentation\n","\n","Realiza rotaciones de las imagenes en 15°, en sentido positivo  y negativo.\n","Se detiene cuando cumple el requisito de cantidad de imagenes.\n","Está configurado para lograr una cantidad de imagenes igual a: `max(#samples por clase) * 1.5`.\n","Es decir, si tenemos un dataset de:\n","```\n","Lunares\t\n","1: BCC   300  ---> 1800\n","2: BKL   600  ---> 1800\n","3: DF     80  ---> 1800\n","4: MEL   600  ---> 1800\n","5: NV   1200  ---> 1800 (el máximo * 50%)\n","6: VASC  110  ---> 1800\n","7: AKIEC 240  ---> 1800 \n","```"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"MzsWuPdQe6sg","executionInfo":{"status":"ok","timestamp":1656985540605,"user_tz":240,"elapsed":509,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["from skimage import transform\n","import shutil\n","\n","\n","class DataAugmentator:\n","  def __init__(self, base_path: str):\n","    self.base_path = base_path\n","    self.subclasses_folders: list[str] = os.listdir(base_path)\n","    self.augment()\n","\n","  def create_rotator(self, angle):\n","    def rotate(img):\n","      return transform.rotate(img, angle=angle)\n","\n","    return rotate\n","  \n","  def augment(self):\n","    img_count = {}\n","\n","    for subfolder in self.subclasses_folders:\n","      img_count[subfolder] = len(os.listdir(self.base_path + subfolder))\n","\n","    target_count = max(img_count.values()) * 3 // 2\n","\n","    strategies = [self.create_rotator(angle) for angle in range(0, 180, 15)]\n","    strategies += [self.create_rotator(-angle) for angle in range(0, 180, 15)]\n","    strategy_generator = self.strategy_generator(strategies)\n","    \n","    for subfolder in self.subclasses_folders:\n","      print(subfolder)\n","\n","      class_folder_path = self.base_path + subfolder + \"/\"\n","      images = os.listdir(class_folder_path)\n","\n","      n_image = img_count[subfolder]\n","      img_pos = 0\n","\n","      if os.path.isdir(subfolder):\n","        shutil.rmtree(subfolder)\n","      os.mkdir(subfolder)\n","\n","      pbar = tqdm(total=target_count - len(images))\n","\n","      while n_image < target_count:\n","        img = LoadImage(class_folder_path + images[img_pos], echo=False)\n","        strat = next(strategy_generator)\n","\n","        with open(f\"{subfolder}/{n_image}.npy\", 'wb') as f:\n","            np.save(f, strat(img))\n","\n","        n_image += 1\n","        pbar.update(1)\n","\n","        if n_image % len(strategies) == 0:\n","          img_pos += 1\n","\n","      pbar.close()\n","    \n","  def strategy_generator(self, strategies):\n","    count = 0\n","    while True:\n","      count += 1\n","\n","      if count == len(strategies):\n","        count = 0\n","    \n","      yield strategies[count]\n","\n","\n","# data_augmentator = DataAugmentator(HOME + \"raw_database/Lunares/\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"HMCZ9K3DlGqS","executionInfo":{"status":"ok","timestamp":1656985540605,"user_tz":240,"elapsed":7,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["from PIL import Image as im"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZJPQQVzHgBAy","executionInfo":{"status":"ok","timestamp":1656985540606,"user_tz":240,"elapsed":7,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["def AugmentAndStore(path: str, datasets: list, root: str='.') -> None:\n","  def Store(store_path: str, root_path: str) -> None:\n","    for aug in tqdm(os.listdir(root_path)):\n","      arr = np.load(f\"{root_path}/{aug}\")\n","      aug, *_ = aug.split('.')\n","      data = im.fromarray(np.uint8(arr * 255), 'L')\n","      data.save(f'{store_path}/{aug}.png')\n","\n","  def GetClasses(path: str) -> list:\n","    out = []\n","    for ds in os.listdir(path):\n","      out += os.listdir(f'{path}/{ds}')\n","    return out\n","\n","  cats = GetClasses(f\"{path}\")\n","  for ds in datasets:\n","    data_augmentator = DataAugmentator(f\"{path}/{ds}/\")\n","    ds_cats = os.listdir(f'{path}/{ds}')\n","    for cat in ds_cats:\n","      if cat in cats:\n","        print(f\"Store {cat}\")\n","        Store(f\"{path}/{ds}/{cat}\", f\"{root}/{cat}\")\n","        "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"inSGsP1xmDkO","outputId":"fbcfa11e-cb5f-4e13-9459-71c989d243ac","executionInfo":{"status":"ok","timestamp":1656985540974,"user_tz":240,"elapsed":374,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["skipping\n"]}],"source":["%%script echo skipping\n","AugmentAndStore(f\"{HOME}raw_database\", [\"Bicicletas\", \"Cachipun\", \"Espinas\", \"Letras\", \"Lunares\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2ev5-_oBLBF"},"outputs":[],"source":["%%script echo skipping\n","datasets = [\"Bicicletas\", \"Cachipun\", \"Espinas\", \"Letras\", \"Lunares\"]\n","\n","for DSNAME in datasets:\n","  if DSNAME == 'Lunares':\n","    factor = 0.5\n","  else:\n","    factor = 1\n","  out = BuildDataset(f\"{HOME}raw_database/{DSNAME}/\", echo=True, min_factor=factor)\n","  _, Ysam, *_ = out\n","  del _\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/Ysam.pkl', 'wb')\n","  pickle.dump(Ysam, pick_insert)\n","  pick_insert.close()\n","  del Ysam"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dss93f4xx_nG","outputId":"df0d8461-e9e2-4fdc-d25d-524ba7377be5","executionInfo":{"status":"ok","timestamp":1656985540976,"user_tz":240,"elapsed":15,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["skipping\n"]}],"source":["%%script echo skipping\n","datasets = [\"Lunares\"]\n","\n","for DSNAME in datasets:\n","  out = BuildDataset(f\"{HOME}raw_database/{DSNAME}/\", echo=True, min_factor=0.5)\n","  Xsam, Ysam, clas, sam = out\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/Ysam.pkl', 'wb')\n","  pickle.dump(Ysam, pick_insert)\n","  pick_insert.close()\n","  del Ysam\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/clas.pkl', 'wb')\n","  pickle.dump(clas, pick_insert)\n","  pick_insert.close()\n","  del clas\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/sam.pkl', 'wb')\n","  pickle.dump(sam, pick_insert)\n","  pick_insert.close()\n","  del sam\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/Xsam.pkl', 'wb')\n","  pickle.dump(Xsam, pick_insert)\n","  pick_insert.close()\n","  del Xsam\n","  "]},{"cell_type":"markdown","metadata":{"id":"DjFNKNButjv0"},"source":["## Bloque 2: Lectura de parámetros\n","Librerías necesarias para este bloque:"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"YdTt6o8Ttq5S","executionInfo":{"status":"ok","timestamp":1656985540976,"user_tz":240,"elapsed":10,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["import json"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ASVOLiYZtr1J","executionInfo":{"status":"ok","timestamp":1656985540977,"user_tz":240,"elapsed":11,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["class Block2:\n","  def __init__(self, path):\n","    '''\n","    Parse params file\n","\n","    INPUT:\n","      path: path to params file\n","    '''\n","    self.path = path\n","    self.load_params()\n","  \n","  def load_params(self):\n","    with open(self.path, encoding = 'utf-8') as file:\n","      params = json.load(file)\n","\n","      self.dataset = params['dataset']\n","      self.features = params['features']\n","      self.transformations = params['transformations']\n","      self.classifiers = []\n","      for c in params['classifiers']:\n","        formatted_input = []\n","        for c_name, c_params in c.items():\n","          formatted_input.append(c_name)\n","          for k, v in c_params.items():\n","            param = \"{0}:{1}\".format(k,v)\n","            formatted_input.append(param)\n","        \n","        self.classifiers.append(\"-\".join(formatted_input))"]},{"cell_type":"markdown","metadata":{"id":"PT7Ak-5Ttud4"},"source":["Ejemplo de uso"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07U1CtedtvjJ","outputId":"9f62ec0f-0d68-46d0-fca0-cf36860d8ecf","executionInfo":{"status":"ok","timestamp":1656985540978,"user_tz":240,"elapsed":10,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["skipping\n"]}],"source":["%%script echo skipping\n","block2 = Block2(f'{HOME}parameters.json')\n","print(block2.classifiers)\n","print(block2.features)\n","print(block2.transformations)\n","print(block2.dataset)\n","del block2 # Importante para no llenar la RAM mientras se prueba"]},{"cell_type":"markdown","metadata":{"id":"7hIGi3eiFUr0"},"source":["## Bloque 3: Extracción de características.\n","Librerías necesarias para este bloque:"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LNBGqeltm6sZ","outputId":"1cefda3c-e55d-4896-aa45-e4ccd1c00f04","executionInfo":{"status":"ok","timestamp":1656985551186,"user_tz":240,"elapsed":10215,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scipy==1.2.0 in /usr/local/lib/python3.7/dist-packages (1.2.0)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.0) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pybalu==0.2.5 in /usr/local/lib/python3.7/dist-packages (0.2.5)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from pybalu==0.2.5) (0.18.3)\n","Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from pybalu==0.2.5) (4.64.0)\n","Requirement already satisfied: scikit-learn>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from pybalu==0.2.5) (1.0.2)\n","Requirement already satisfied: imageio>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from pybalu==0.2.5) (2.19.3)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pybalu==0.2.5) (1.2.0)\n","Requirement already satisfied: numpy>=1.16.1 in /usr/local/lib/python3.7/dist-packages (from pybalu==0.2.5) (1.21.6)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio>=2.5.0->pybalu==0.2.5) (9.2.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->pybalu==0.2.5) (2021.11.2)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->pybalu==0.2.5) (3.2.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->pybalu==0.2.5) (2.6.3)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->pybalu==0.2.5) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14.2->pybalu==0.2.5) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14.2->pybalu==0.2.5) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14.2->pybalu==0.2.5) (1.4.3)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14.2->pybalu==0.2.5) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14.2->pybalu==0.2.5) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14.2->pybalu==0.2.5) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->pybalu==0.2.5) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->pybalu==0.2.5) (3.1.0)\n"]}],"source":["#%%script echo skipping\n","from IPython.display import clear_output\n","!pip3 install scipy==1.2.0\n","!pip3 install pybalu==0.2.5"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"W7z6etBgILKp","executionInfo":{"status":"ok","timestamp":1656985552155,"user_tz":240,"elapsed":982,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["from pybalu.feature_extraction import hog_features, lbp_features, haralick_features, gabor_features, basic_int_features"]},{"cell_type":"markdown","metadata":{"id":"yXaj5rg6lfuZ"},"source":["Formato del nombre para cada característica soportada:\n","  - HoG: \"HOG-NxMxB\"\n","  - LBP: \"LBP-NxM\"\n","  - Haralick: \"HAR-N\"\n","  - Gabor: \"GAB-RxD\"\n","  - Basic intensity: \"BASIC-\"\n","\n","Formato de archivo de características guardado en Drive:\n","\n","\"DSNAME_feature1+feature2.pkl\"\n","\n","Ej:\n","\"Bicicletas_LBP-5x5+HOG-7x7x9.pkl\""]},{"cell_type":"code","execution_count":17,"metadata":{"id":"mg2U1ZGol_ZA","executionInfo":{"status":"ok","timestamp":1656985552569,"user_tz":240,"elapsed":418,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["class Block3:\n","  def __init__(self, DSNAME, features, load_database=True, load_features=True, save=True):\n","      '''\n","      Extracts the features indicated from the DSNAME dataset.\n","\n","      INPUT:\n","        DSNAME: name of the dataset stored in Drive.\n","        features: array of strings following feature name convention,\n","        load_database: True -> loads pickled images directly.\n","        load_features: True -> tries to load the features from Drive in case they already\n","          been extracted before. If file does not exist, the features are extracted.\n","        save:          True -> saves the extracted features to Drive.\n","\n","      OUTPUT:\n","        Feature matrix is stored in the variable self.X.\n","        Ground truth is stored in the variable self.Ysam\n","      '''\n","      self.DSNAME = DSNAME\n","      self.features = features\n","      self.features_names, self.features_parameters = self.parse_features()\n","      self.save = save\n","\n","      if load_database:\n","        self.Xsam, self.Ysam, self.n_class, self.n_samples = self.import_pickled_dataset()\n","      else:\n","        self.Xsam, self.Ysam, self.n_class, self.n_samples = BuildDataset(f\"{HOME}raw_database/{self.DSNAME}/\", echo=True)\n","      self.n_class += 1\n","\n","      self.identifier = self.DSNAME + \"_\" + \"+\".join(self.features)\n","      if load_features:\n","        self.X = self.load_features()\n","      else:\n","        self.X = self.extract_features()\n","  \n","  def parse_features(self):\n","    features_names = []\n","    features_parameters = dict()\n","\n","    for feature in self.features:\n","      name = feature.split(\"-\")[0]\n","      parameters = feature.split(\"-\")[1]\n","      if name == \"HOG\":\n","        parameters = [int(param) for param in parameters.split(\"x\")]\n","\n","      elif name == \"LBP\":\n","        parameters = [int(param) for param in parameters.split(\"x\")]\n","\n","      elif name == \"HAR\":\n","        parameters = [int(parameters)]\n","      \n","      elif name == \"GAB\":\n","        parameters = [int(param) for param in parameters.split(\"x\")]\n","      \n","      elif name == \"BASIC\":\n","        parameters = []\n","\n","      features_names.append(name)\n","      features_parameters[name] = parameters\n","    \n","    return features_names, features_parameters\n","    \n","\n","  def import_pickled_dataset(self):\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/Xsam.pkl','rb')\n","    Xsam = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/Ysam.pkl','rb')\n","    Ysam = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/clas.pkl','rb')\n","    n_class = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/sam.pkl','rb')\n","    n_samples = pickle.load(file_read)\n","    file_read.close()\n","    return Xsam, Ysam, n_class, n_samples\n","\n","  def load_features(self):\n","    try:\n","      read_file = open(f'{HOME}pickled_features/{self.DSNAME}/{self.identifier}.pkl', 'rb')\n","      X = pickle.load(read_file)\n","      return X\n","    except:\n","      print(f'Could not load {self.identifier}.pkl')\n","      X = self.extract_features()\n","      return X\n","\n","  def extract_features(self):\n","    total_images = sum(self.n_samples)\n","    feature_matrices = dict() # Dictionary indexed by feature name that saves feature matrix of feature.\n","\n","    # Initialize feature matrices\n","    for feature in self.features_names:\n","      if feature == \"LBP\":\n","        parameters = self.features_parameters[feature]\n","        M = 59*parameters[0]*parameters[1]\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","\n","      elif feature == \"HOG\":\n","        parameters = self.features_parameters[feature]\n","        M = parameters[0]*parameters[1]*parameters[2]\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","      \n","      elif feature == \"HAR\":\n","        M = 28\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","    \n","      elif feature == \"GAB\":\n","        parameters = self.features_parameters[feature]\n","        M = parameters[0]*parameters[1] + 3\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","\n","      elif feature == \"BASIC\":\n","        M = 5\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","\n","    # Extract each feature from each image.\n","    t = 0\n","    for image in self.Xsam:\n","      for feature in self.features_names:\n","        if feature == \"LBP\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = lbp_features(image, hdiv=parameters[0], vdiv=parameters[1], mapping='nri_uniform')\n","\n","        elif feature == \"HOG\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = hog_features(image, v_windows=parameters[0], h_windows=parameters[1], n_bins=parameters[2])\n","\n","        elif feature == \"HAR\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = haralick_features(image, distance=parameters[0])\n","\n","        elif feature == \"GAB\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = gabor_features(image, rotations=parameters[0], dilations=parameters[1])\n","\n","        elif feature == \"BASIC\":\n","          feature_matrices[feature][t,:] = basic_int_features(image)[0:5]\n","\n","      t+=1\n","    \n","    # Concatenate features\n","    X = np.concatenate(tuple([feature_matrices[feature] for feature in self.features_names]), axis=1)\n","\n","    if self.save:\n","      try:\n","        pick_insert = open(f'{HOME}pickled_features/{self.DSNAME}/{self.identifier}.pkl', 'wb')\n","        pickle.dump(X, pick_insert)\n","        pick_insert.close()\n","      except:\n","        print(f'Could not save {self.identifier}.pkl')\n","\n","    return X\n","\n","  def __str__(self):\n","    return self.identifier"]},{"cell_type":"markdown","metadata":{"id":"KnKiJy0pJ2zy"},"source":["Ejemplo de uso"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4YZjirdqKez","outputId":"d82774c3-89f1-47f6-90a2-a2f3d5a67e17","executionInfo":{"status":"ok","timestamp":1656985552570,"user_tz":240,"elapsed":8,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["skipping\n"]}],"source":["%%script echo skipping\n","block3 = Block3(\"Lunares\", [\"LBP-3x3\", \"HOG-3x3x9\", \"HAR-1\", \"GAB-2x2\", \"BASIC-\"], load_database=True, load_features=False, save=True)\n","\n","print(block3)\n","print(block3.features_names, block3.features_parameters)\n","print(block3.Xsam.shape, block3.Ysam.shape, block3.n_class, block3.n_samples)\n","print(block3.X.shape)\n","\n","del block3 # Importante para no llenar la RAM mientras se prueba"]},{"cell_type":"markdown","metadata":{"id":"N1flNF6VEERS"},"source":["## Bloque 4: selección y transformación de características.\n","- Split Train-Validation\n","- Aplicación secuencial de alguna selección/transformación\n","  - Clean\n","  - MinMax Scaling\n","  - SelectKBest\n","  - SFS\n","  - PCA\n","  - ICA"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"NVNNFHwbKdeh","executionInfo":{"status":"ok","timestamp":1656985552572,"user_tz":240,"elapsed":8,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from pybalu.feature_selection import clean\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.feature_selection import SelectKBest, chi2, f_classif\n","from pybalu.feature_selection import sfs\n","from sklearn.decomposition import PCA, FastICA"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"wPSGbmLbOndK","executionInfo":{"status":"ok","timestamp":1656985552816,"user_tz":240,"elapsed":251,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["class CleanInterface:\n","\n","  def __init__(self, X):\n","    self.model = clean(X)\n","\n","  def transform(self, X):\n","    return X[:, self.model]\n","\n","class SFSInterface:\n","\n","  def __init__(self, X, y, s):\n","    self.model = sfs(X, y, s, show=False)\n","\n","  def transform(self, X):\n","    return X[:, self.model]"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"qYdP2hn7GEeo","executionInfo":{"status":"ok","timestamp":1656985552816,"user_tz":240,"elapsed":6,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["class Block4:\n","\n","  def __init__(self, X, y, sequence, ratio=0.3):\n","\n","    self.sequence = sequence\n","\n","    self.models = []\n","\n","    self.Xtrain, self.Xval, self.ytrain, self.yval = train_test_split(np.array(X), y, test_size=ratio, random_state=42, stratify=y)\n","\n","    self.Xtrain = self.interative_fit()\n","    self.Xval = self.transform(self.Xval)\n","\n","  def interative_fit(self):\n","\n","    X = self.Xtrain\n","\n","    for seq in self.sequence:\n","\n","      name = seq.split('-')[0]\n","\n","      if name == 'CLEAN':\n","        model = CleanInterface(X)\n","      elif name == 'MINMAX':\n","        model = MinMaxScaler().fit(X)\n","      else: \n","        param = int(seq.split('-')[1])\n","        if name == 'KBEST':\n","          model = SelectKBest(f_classif, k=param).fit(X, self.ytrain.flatten())\n","        elif name == 'SFS':\n","          model = SFSInterface(X, self.ytrain, param)\n","        elif name == 'PCA':\n","          model = PCA(n_components=param).fit(X)\n","        elif name == 'ICA':\n","          model = FastICA(n_components=param, random_state=0).fit(X, self.ytrain)\n","        else:\n","          model = None\n","          print(f'No existe el modelo {name}')\n","      \n","      \n","      self.models.append(model)\n","      X = model.transform(X)\n","\n","    return X\n","  \n","  def transform(self, X):\n","\n","    for model in self.models:\n","      X = model.transform(X)\n","    return X"]},{"cell_type":"markdown","metadata":{"id":"qpFT1cz9yTAa"},"source":["Ejemplo de uso"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3N8G2DVGJiA","outputId":"af150117-1bb0-4b69-bf4e-f4200d6f951d","executionInfo":{"status":"ok","timestamp":1656985552817,"user_tz":240,"elapsed":6,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["skipping\n"]}],"source":["%%script echo skipping\n","!gdown --id 1CA-l9_JjdjG_4kTuavKf8Wm27dt0jyqT\n","clear_output()\n","f = open('data.p', \"rb\")\n","data = pickle.load(f)\n","X = data['train']\n","y = np.array([0 if i < 7000 else 1 for i in range(0, 14000)])\n","X.shape, len(y)\n","\n","block = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-50', 'SFS-10', 'PCA-5', 'ICA-2'])\n","block.Xtrain.shape, block.Xval.shape"]},{"cell_type":"markdown","metadata":{"id":"il1epoYouaXm"},"source":["## Bloque 5"]},{"cell_type":"markdown","metadata":{"id":"sWcojGo9uzEL"},"source":["<div align=\"center\">\n","<img src=\"https://i.imgur.com/jxpf9U9.png\"></img>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"9AWMQRQEu9m4"},"source":["### Modo de uso\n","\n","**Block5 recibe dos elementos:**\n","\n","- **blocks**: lista de instancias de la clase `Block4`\n","- **sequence**: lista de strings\n","\n","Para sequence, un elemento de la lista puede verse así:\n","\n","`FOREST-max_depth:3-n_estimators:100`\n","\n","Es decir: `{CLASSIFIER_NAME}-{PARAM_1_NAME}:{PARAM_1_VALUE}-{PARAM_2_NAME}:{PARAM_2_VALUE}`\n","\n","Puede que un Clasificador no tenga parámetros (como `QDA`). En ese caso basta sólo con el nombre.\n","\n","El mapping de nombre --> clasificador se aprecia en `self.classifiers` de `Block5`. Ahí mismo, se aprecian los `kwargs`, es decir los parámetros que recibe el constructor de dicho clasificador.\n","\n","Kwargs son los parámetros permitidos a personalizar. Lo demás será default.\n","Hay casos donde estos parámetros son obligatorios, y otros donde no.\n","A modo de maximizar el accuracy y evitar errores, ojalá todos estén presentes (mayor personalización).\n","\n","Al momento de instanciar el bloque, inmediatamente se comienza el proceso de búsqueda y validación del mejor modelo.\n","Al terminar, en el atributo `best_classifier` queda guardado el mejor clasificador. Es decir, una tupla de:\n","\n","`final_accuracy: float, <block4>: Block4, clf_class: object, params: dict`"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"yon3u8FHub7l","executionInfo":{"status":"ok","timestamp":1656985552818,"user_tz":240,"elapsed":5,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n","\n","from sklearn.svm import SVC\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import cross_val_score"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"lRwfLiwRvIdU","executionInfo":{"status":"ok","timestamp":1656985553058,"user_tz":240,"elapsed":244,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["from typing import List\n","\n","class Params:\n","    N_FOLDS = 10\n","\n","class Block5:\n","\n","  def __init__(self, blocks: List['Block4'], sequence: List[str]):\n","\n","    self.blocks = blocks\n","    self.sequence = sequence\n","\n","    # Kwargs son los parámetros permitidos a personalizar. Lo demás será default.\n","    # Hay casos donde estos parámetros son obligatorios, y otros donde no.\n","    # A modo de maximizar el accuracy y evitar errores, ojalá todos estén presentes (mayor personalización).\n","    self.classifiers = {\n","          \"KNN\": {\"clf\": KNeighborsClassifier, \"kwargs\": [\"n_neighbors\"]},\n","          \"DMIN\": {\"clf\": NearestCentroid, \"kwargs\": []},\n","          \"SVC\": {\"clf\": SVC, \"kwargs\": [\"kernel\", \"C\", \"gamma\"]},\n","          \"TREE\": {\"clf\": DecisionTreeClassifier, \"kwargs\": [\"max_depth\"]},\n","          \"FOREST\": {\"clf\": RandomForestClassifier, \"kwargs\": [\"max_depth\", \"n_estimators\"]},\n","          \"NBAYES\": {\"clf\": GaussianNB, \"kwargs\": []},\n","          \"QDA\": {\"clf\": QuadraticDiscriminantAnalysis, \"kwargs\": []},\n","    }\n","\n","    self.best_classifier = self.interative_classify()\n","\n","  def interative_classify(self):\n","\n","    N = {}\n","\n","    for block in self.blocks:\n","      for seq in self.sequence:\n","        \n","        # \"FOREST-max_depth:3-n_estimators:100\" --> [\"FOREST\", \"max_depth:3-n_estimators:100\"]\n","        splitted_seq = seq.split('-')\n","        # name = \"FOREST\"\n","        name = splitted_seq[0]\n","        # params = [[\"max_depth\", \"3\"], [\"n_estimators\", \"100\"]]\n","        params = [clf.split(\":\") for clf in splitted_seq[1:]] if len(splitted_seq) > 1 else []\n","\n","        clf_info = self.classifiers[name]\n","        # RandomForestClassifier\n","        clf_class = clf_info[\"clf\"]\n","\n","        if name == \"SVC\": # necesita tipos de datos especiales\n","          param_types = {\"kernel\": str, \"C\": float, \"gamma\": int}\n","          params = {param_name: param_types[param_name](param_value) for param_name, param_value in params}\n","        else: # en caso contrario, todo parámetro se trata como <int>\n","          params = {param_name: int(param_value) for param_name, param_value in params}\n","\n","        crossval_score = self.cross_validation(block.Xtrain, block.ytrain, clf_class, params)\n","        N[crossval_score] = (block, clf_class, params, seq)\n","\n","    # se obtiene el clasificador con mayor score en cross_validation\n","    max_n = max(N.keys())\n","    block, clf_class, params, seq = N[max_n]\n","\n","    final_accuracy = self.hold_out(block.Xtrain, block.ytrain,\n","                                   block.Xval, block.yval,\n","                                   clf_class, params)\n","    \n","    clf = clf_class(**params)\n","\n","    clf.fit(block.Xtrain, block.ytrain.flatten())\n","\n","\n","    return (final_accuracy, block, clf_class, params, seq, clf)\n","\n","\n","  def hold_out(self, X_train, y_train, X_test, y_test, clf, params) -> float:\n","    \"\"\" Retorna accuracy score al entrenar el clasificador en Train y probarlo en Test\"\"\"\n","    clf = clf(**params)\n","\n","    fitted_clf = clf.fit(X_train, y_train.flatten())\n","\n","    y_pred = fitted_clf.predict(X_test)\n","\n","    return accuracy_score(y_test, y_pred)\n","\n","  def cross_validation(self, X_train, y_train, clf, params, n_folds=Params.N_FOLDS) -> float:\n","    \"\"\" Retorna score de validación cruzada\"\"\"\n","    cv_scores = cross_val_score(clf(**params), X_train, y_train.flatten(), cv=n_folds)\n","    return cv_scores.mean()"]},{"cell_type":"markdown","metadata":{"id":"nsMB2dhIvJuB"},"source":["Ejemplo de uso"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEDpLOPovKuh","outputId":"06a81df0-868d-4e03-b238-cbdf833bf1fd","executionInfo":{"status":"ok","timestamp":1656985553059,"user_tz":240,"elapsed":6,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["skipping\n"]}],"source":["%%script echo skipping\n","import pickle\n","\n","\n","def dump_sample_block4():\n","  with open('block4_A.pkl', 'wb') as handle:\n","      pickle.dump(block_A, handle)\n","  \n","  with open('block4_B.pkl', 'wb') as handle:\n","    pickle.dump(block_B, handle)\n","\n","  with open('block4_C.pkl', 'wb') as handle:\n","    pickle.dump(block_C, handle)\n","\n","\n","def load_sample_block4():\n","  base_path: str = \"/content/drive/MyDrive/home/sample_data/\"\n","\n","  with open(base_path +'block4_A.pkl', 'rb') as handle:\n","    block_A = pickle.load(handle)\n","\n","  with open(base_path + 'block4_B.pkl', 'rb') as handle:\n","    block_B = pickle.load(handle)\n","\n","  with open(base_path + 'block4_C.pkl', 'rb') as handle:\n","    block_C = pickle.load(handle)\n","\n","  return (block_A, block_B, block_C)\n","\n","#block_A = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-50', 'SFS-10', 'PCA-5', 'ICA-2'])\n","#block_B = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-20', 'SFS-10', 'PCA-5'])\n","#block_C = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-20', 'ICA-2'])\n","\n","block_A, block_B, block_C = load_sample_block4()\n","BLOCKS = [block_A, block_B, block_C]\n","\n","results = Block5(BLOCKS, [\"FOREST-max_depth:3-n_estimators:100\", \"KNN-n_neighbors:15\",\n","                          \"NBAYES\", \"SVC-kernel:linear-C:0.025-gamma:2\"])\n","\n","results.best_classifier"]},{"cell_type":"markdown","metadata":{"id":"jPsPu_hBy9Hk"},"source":["# AutoPR"]},{"cell_type":"markdown","metadata":{"id":"Mk518Bhm7DyL"},"source":["Convención:\n","\n","\\[DATASET]\\_\\[feature1+feature2+...\\]\\_\\[transf1+transf2]\\_\\[classifier-param1name:param1value-param2name:param2value\\]\n"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"mZLnzQYzy8d0","executionInfo":{"status":"ok","timestamp":1656987293369,"user_tz":240,"elapsed":894,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["class AutoPR:\n","  def __init__(self, path, load_database=True, load_features=True, save=False):\n","    self.parameters_path = path\n","    self.load_database = load_database\n","    self.load_features = load_features\n","    self.save = save\n","    # (CV_final_accuracy, block4, clf_class, params, seq, clf_instance)\n","    self.best_classifier_data, self.block2 = self.main()\n","    self.identifier = self.block2.dataset + \"_\" + \"+\".join(self.block2.features) + \"_\" + \"+\".join(self.best_classifier_data[1].sequence) + \"_\" + self.best_classifier_data[4]\n","\n","    if self.save:\n","      self.export_classifier()\n","  \n","  def main(self):\n","    self.block2 = Block2(self.parameters_path)\n","\n","    self.block3 = Block3(\n","        self.block2.dataset, self.block2.features, load_database=self.load_database, load_features=self.load_features, save=self.save\n","    )\n","\n","    block4_list = []\n","    for transformation in self.block2.transformations:\n","      self.block4 = Block4(self.block3.X, self.block3.Ysam, transformation)\n","      block4_list.append(self.block4)\n","    \n","    self.block5 = Block5(block4_list, self.block2.classifiers)\n","\n","    return self.block5.best_classifier, self.block2\n","  \n","  def predict(self, X, scale=1.0):\n","    # Extraer características a partir de block1.features\n","    features=self.block2.features\n","    feature_names=self.block3.features_names\n","    feature_params=self.block3.features_parameters\n","    def extract_features(X, features, feature_names, feature_params):\n","      # esto es exactamente lo mismo que hay en Block3, pero no puedo \n","      # heredar/sobreescribirlo sin matar el BuildDataset porque tiene\n","      # hardocdeado el path para cargar un DS_raw y no el de demo\n","      total_images = X.shape[0]\n","      feature_matrices = dict() # Dictionary indexed by feature name that saves feature matrix of feature.\n","\n","      # Initialize feature matrices\n","      for feature in feature_names:\n","        if feature == \"LBP\":\n","          parameters = feature_params[feature]\n","          M = 59*parameters[0]*parameters[1]\n","          Xfeat = np.zeros((total_images,M))\n","          feature_matrices[feature] = Xfeat\n","\n","        elif feature == \"HOG\":\n","          parameters = feature_params[feature]\n","          M = parameters[0]*parameters[1]*parameters[2]\n","          Xfeat = np.zeros((total_images,M))\n","          feature_matrices[feature] = Xfeat\n","        \n","        elif feature == \"HAR\":\n","          M = 28\n","          Xfeat = np.zeros((total_images,M))\n","          feature_matrices[feature] = Xfeat\n","      \n","        elif feature == \"GAB\":\n","          parameters = feature_params[feature]\n","          M = parameters[0]*parameters[1] + 3\n","          Xfeat = np.zeros((total_images,M))\n","          feature_matrices[feature] = Xfeat\n","\n","        elif feature == \"BASIC\":\n","          M = 5\n","          Xfeat = np.zeros((total_images,M))\n","          feature_matrices[feature] = Xfeat\n","\n","      # Extract each feature from each image.\n","      t = 0\n","      for image in X:\n","        for feature in feature_names:\n","          if feature == \"LBP\":\n","            parameters = feature_params[feature]\n","            feature_matrices[feature][t,:] = lbp_features(image, hdiv=parameters[0], vdiv=parameters[1], mapping='nri_uniform')\n","\n","          elif feature == \"HOG\":\n","            parameters = feature_params[feature]\n","            feature_matrices[feature][t,:] = hog_features(image, v_windows=parameters[0], h_windows=parameters[1], n_bins=parameters[2])\n","\n","          elif feature == \"HAR\":\n","            parameters = feature_params[feature]\n","            feature_matrices[feature][t,:] = haralick_features(image, distance=parameters[0])\n","\n","          elif feature == \"GAB\":\n","            parameters = feature_params[feature]\n","            feature_matrices[feature][t,:] = gabor_features(image, rotations=parameters[0], dilations=parameters[1])\n","\n","          elif feature == \"BASIC\":\n","            feature_matrices[feature][t,:] = basic_int_features(image)[0:5]\n","\n","        t+=1\n","      \n","      # Concatenate features\n","      X = np.concatenate(tuple([feature_matrices[feature] for feature in feature_names]), axis=1)\n","\n","      return X\n","\n","    X = extract_features(X, features, feature_names, feature_params)\n","\n","    # Aplicar transfomaciones a la imagen\n","    # Aplicar self.best_classifier_data[1].transform()\n","    X = self.best_classifier_data[1].transform(X)\n","\n","    # Hacer self.best_classifier_data[4].predict()\n","    X = self.best_classifier_data[5].predict(X)\n","\n","    return X\n","\n","  def export_classifier(self):\n","      try:\n","        pick_insert = open(f'{HOME}trained_models/{self.block2.dataset}/{self.identifier}.pkl', 'wb')\n","        pickle.dump(self, pick_insert)\n","        pick_insert.close()\n","      except Exception as e:\n","        print(f\"Exception: {e}\")\n","        print(f'Could not save {self.identifier}.pkl')\n","    "]},{"cell_type":"markdown","metadata":{"id":"hvnKwuz82Hc0"},"source":["Ejemplo de uso: hiperbúsqueda"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cFT2EIhbvD-","outputId":"e13008c1-00fc-4b81-db90-37a76bf4daed","executionInfo":{"status":"ok","timestamp":1656985553374,"user_tz":240,"elapsed":6,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["skipping\n"]}],"source":["%%script echo skipping\n","import json\n","import random\n","\n","with open(f'{HOME}parameters/espinas1.json') as f:\n","  data = json.load(f)\n","  MAX = 0\n","\n","  for i in range(1, 20):\n","    with open(\"tmp.json\", \"w\") as tmp:\n","      pca1 = random.randint(1, 500)\n","      k1 = random.randint(1, pca1)\n","      ica1 = random.randint(1, k1)\n","      ica2 = random.randint(1, pca1)\n","\n","      knn1 = (i * 2)\n","      tree1 = (i * 3)\n","      depth1 = (i * 2)\n","      estimators1 = (i * 15)\n","\n","\n","      # data[\"transformations\"] = [[\"MINMAX\", f\"KBEST-287\", f\"PCA-247\"],\n","      #                            ]\n","\n","      data[\"classifiers\"] = [\n","                          {\n","                              \"KNN\": {\"n_neighbors\": knn1}\n","                          },\n","                          {\n","                              \"TREE\": {\"max_depth\": tree1}\n","                             },\n","                          {\n","                              \"FOREST\": {\"max_depth\": depth1,\"n_estimators\": estimators1}\n","                          },\n","                          {\n","                              \"NBAYES\": {}\n","                          },\n","                          {\n","                              \"QDA\": {}\n","                          },\n","                          {\n","                              \"DMIN\": {}\n","                          }\n","      ]\n","      json.dump(data, tmp)\n","      \n","    autopr = AutoPR(\"tmp.json\", load_database=True, load_features=True, save=True)\n","    acc = autopr.best_classifier_data[0]\n","    if acc > MAX:\n","      MAX = acc\n","      print(acc)\n","      print(autopr.identifier, \"\\n\")\n","\n","  \n","# 0.8415147265077139\n","# Bicicletas_HOG-8x12x9+LBP-10x10_MINMAX+KBEST-287+PCA-247_SVC-kernel:rbf-C:1\n","\n","# 0.8923611111111112\n","# Espinas_HOG-5x5x9+LBP-8x8+HAR-20+BASIC-_MINMAX+SFS-8_FOREST-max_depth:10-n_estimators:75 "]},{"cell_type":"code","execution_count":78,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"NAxZyaWy2ITK","outputId":"29f2f29e-5b17-4024-a848-d11838a321a8","executionInfo":{"status":"ok","timestamp":1656989007547,"user_tz":240,"elapsed":12700,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0.9117647058823529\n"]},{"output_type":"execute_result","data":{"text/plain":["'Letras_HOG-16x16x9+LBP-10x10_CLEAN+MINMAX+KBEST-50_FOREST-max_depth:10-n_estimators:100'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":78}],"source":["autopr = AutoPR(f'{HOME}parameters/parameters.json', load_database=True, load_features=True, save=True)\n","# si tira error, correr una vez con load_features=False\n","print(autopr.best_classifier_data[0])\n","autopr.identifier"]},{"cell_type":"markdown","metadata":{"id":"9YfvKb_pjkbz"},"source":["# Log AutoPR"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"lS8wL6dHnpRU","executionInfo":{"status":"ok","timestamp":1656988692301,"user_tz":240,"elapsed":243,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["import csv"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"UGNRfTSFjpNJ","executionInfo":{"status":"ok","timestamp":1656988693222,"user_tz":240,"elapsed":4,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[],"source":["def autopr_data_to_csv(autopr: AutoPR, path: str=f'{HOME}results.csv', echo=False):\n","  with open(path, 'a', newline='') as fil:\n","    acc, *rest, clas = autopr.best_classifier_data\n","    row = [autopr.block2.dataset, autopr.block2.features, autopr.best_classifier_data[1].sequence, clas, acc, autopr.identifier]\n","    if echo:\n","      print(row)\n","    writer = csv.writer(fil, delimiter=',')\n","    writer.writerow(row)"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DBkcC438yJB8","outputId":"04e55d67-6e80-4d74-b26b-000e80c9c05d","executionInfo":{"status":"ok","timestamp":1656988694678,"user_tz":240,"elapsed":321,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['Letras', ['HOG-16x16x9', 'LBP-10x10'], ['CLEAN', 'MINMAX', 'KBEST-50'], RandomForestClassifier(max_depth=10), 0.9117647058823529, 'Letras_HOG-16x16x9+LBP-10x10_CLEAN+MINMAX+KBEST-50_FOREST-max_depth:10-n_estimators:100']\n"]}],"source":["#%%script echo skipping\n","autopr_data_to_csv(autopr, echo=True)"]},{"cell_type":"markdown","metadata":{"id":"i4dWMKD9BLBc"},"source":["# Demo predict"]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g4lJr5uWBLBc","executionInfo":{"status":"ok","timestamp":1656989142779,"user_tz":240,"elapsed":2513,"user":{"displayName":"Tomás Contreras","userId":"11462373198933620104"}},"outputId":"36efac30-addb-4419-f60b-0516db5dadc7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0., 5., 0.],\n","       [2., 0., 0., 3., 0.],\n","       [1., 0., 0., 1., 0.],\n","       [1., 0., 0., 3., 0.],\n","       [2., 0., 0., 2., 0.],\n","       [3., 0., 0., 2., 0.],\n","       [3., 0., 0., 4., 0.],\n","       [2., 0., 0., 1., 0.],\n","       [2., 0., 0., 5., 0.],\n","       [2., 0., 0., 4., 0.]])"]},"metadata":{},"execution_count":79}],"source":["# dsname es el nombre del dataset\n","# Version es el nombre del pkl que obtuvo el mejor rendimiento\n","models = [\n","    {\"dsname\": \"Bicicletas\", \"Version\": 'Bicicletas_HOG-8x12x9+LBP-10x10_MINMAX+KBEST-287+PCA-247_SVC-kernel:rbf-C:1'},\n","    {\"dsname\": \"Cachipun\", \"Version\": None},\n","    {\"dsname\": \"Espinas\", \"Version\": None},\n","    {\"dsname\": \"Letras\", \"Version\": 'Letras_HOG-16x16x9+LBP-10x10_CLEAN+MINMAX+KBEST-50_FOREST-max_depth:10-n_estimators:100'},\n","    {\"dsname\": \"Lunares\", \"Version\": None},\n","]\n","\n","demo_path = f'{HOME}demo_data/'\n","\n","out = np.zeros([10, 5])\n","\n","for i, model in enumerate(models):\n","    if model['Version']:\n","      pick_insert = open(f'{HOME}trained_models/{model[\"dsname\"]}/{model[\"Version\"]}.pkl', 'rb')\n","      autopr = pickle.load(pick_insert)\n","      pick_insert.close()\n","      pick_inser = open(f'{HOME}pickled_database/{model[\"dsname\"]}/imdim.pkl', 'rb')\n","      imdim = pickle.load(pick_inser)\n","      pick_inser.close()\n","      images = [None for x in range(10)]\n","      path = sorted(os.listdir(f'{demo_path}data0{i+1}'))\n","      for ii, impath in enumerate(path):\n","          img = LoadImage(f'{demo_path}/data0{i+1}/{impath}', echo=False)\n","          img = cv2.resize(img, imdim, interpolation = cv2.INTER_AREA) \n","          images[ii] = img\n","      preds = autopr.predict(np.asarray(images))\n","      for ii in range(len(preds)):\n","          out[ii, i] = preds[ii] # orden i, ii?\n","    else:\n","      continue\n","\n","with open(f'{HOME}C_TOTOCOCA.npy', 'wb') as fil:\n","    np.save(fil, out)\n","\n","out"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"base.ipynb","provenance":[],"toc_visible":true},"interpreter":{"hash":"6189f557f5bc4df41e92f7a00b3721572a07e07697aeafd1cb02e690e443f281"},"kernelspec":{"display_name":"Python 3.9.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
