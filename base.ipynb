{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"j2A7EsDgUZWE"},"outputs":[],"source":["import cv2\n","import os\n","import pickle\n","import numpy as np\n","import PIL as pil\n","from tqdm.auto import tqdm"]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQjqoOMgV4TI","outputId":"db16bd5d-64d4-4256-c1d6-ad6e68acc7fd","executionInfo":{"status":"ok","timestamp":1655757480092,"user_tz":240,"elapsed":24527,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Drive files\n","\n","1. Aceptar invitación a la unidad compartida Patrones2022\n","\n","2. Abrir la unidad, y sobre la carpeta `home`, hacer click derecho y luego \"Añadir acceso directo a Drive\".\n","\n","Si eso funcionó, al ejecutar la siguiente celda, debería verse `db_caracteristicas raw_database  trained_models` en el output."],"metadata":{"id":"culpx2NQWV_H"}},{"cell_type":"code","source":["HOME = \"/content/drive/My Drive/home/\"\n","!ls \"{HOME}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Sq2Th8bWRgp","outputId":"e0d7b8d3-2630-40ea-e3fb-d460fad764d5","executionInfo":{"status":"ok","timestamp":1655757480872,"user_tz":240,"elapsed":788,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["db_caracteristicas  pickled_database  raw_database  trained_models\n"]}]},{"cell_type":"code","source":["!ls \"{HOME}/raw_database\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cki2PHBWyK_k","outputId":"6e930b10-61f2-4413-a589-860bd51a745b","executionInfo":{"status":"ok","timestamp":1655757481319,"user_tz":240,"elapsed":451,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bicicletas  Cachipun  Espinas  Letras  Lunares\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5ZXIY4JUZWL"},"outputs":[],"source":["def LoadImage(path: str, cmap: str = 'gray', echo: bool = True) -> np.ndarray:\n","    \"\"\"\n","    Load an image from a path\n","    \"\"\"\n","    if cmap == 'gray':\n","      cflag = cv2.IMREAD_GRAYSCALE\n","    elif cmap == 'rgb':\n","      cflag = cv2.IMREAD_COLOR\n","    else:\n","      print(f\"{cmap} is not a valid option\")\n","      raise AttributeError\n","    if echo:\n","      print(\"Image: \" + path)\n","    img = cv2.imread(path, cflag)\n","    if echo:\n","      print(\"Image size:\", img.shape)\n","    return img\n","\n","\n","def NofClasses(path: str) -> int:\n","    \"\"\"\n","    Get the classes in a directory\n","    \"\"\"\n","    return len(os.listdir(path)) - 1\n","\n","\n","def NofSamples(path: str) -> list:\n","    \"\"\"\n","    Get the number of samples for each class\n","    \"\"\"\n","    samples = []\n","    for subdir in os.listdir(path):\n","        samples.append(len(os.listdir(path + subdir)))\n","    return samples\n","\n","\n","def GetMinDim(path: str) -> tuple:\n","  \"\"\"\n","  Returns the smallest dimensions from every image in path.\n","  Path must be a nonempty folder, with at least one folder with images.\n","  \"\"\"\n","  minh = None\n","  minw = None\n","  for dir in os.listdir(path):\n","    for fil in os.listdir(path+dir):\n","      h, w = LoadImage(path + dir + '/' + fil, cmap=cmap, echo=echo).shape\n","      if not minh or h < minh:\n","        minh = h\n","      if not minw or w < minw:\n","        minw = w\n","      break\n","  return (minw, minh)\n","\n","\n","def BuildDataset(path: str, cmap: str = 'gray', echo: bool = False) -> tuple:\n","    \"\"\"\n","    Build a dataset from a directory, returns a tuple (X, y, #clas, [#sam])\n","    \"\"\"\n","    imdim = GetMinDim(path)\n","    if echo:\n","      print(f\"Smallest image size: {imdim}\")\n","    classes = NofClasses(path)\n","    samples = NofSamples(path)\n","    Xsam = np.zeros((sum(samples), imdim[1], imdim[0]))\n","    Ysam = np.zeros((sum(samples), 1))\n","    i = 0\n","    ii = 0\n","    echo = True\n","    for dir in os.listdir(path):\n","        for fil in tqdm(os.listdir(path + dir)):\n","            if fil == '.DS_Store':\n","              continue\n","            img = LoadImage(path + dir + '/' + fil, cmap=cmap, echo=echo)\n","            img = cv2.resize(img, imdim, interpolation = cv2.INTER_AREA) \n","            Xsam[ii] = img\n","            echo = False\n","            Ysam[ii] = i\n","            ii += 1\n","        i += 1\n","    return (Xsam, Ysam, classes, samples)\n","    "]},{"cell_type":"code","source":["%%script echo skipping\n","\n","datasets = [\"Bicicletas\", \"Cachipun\", \"Espinas\", \"Letras\", \"Lunares\"]\n","\n","for DSNAME in datasets:\n","  out = BuildDataset(f\"{HOME}raw_database/{DSNAME}/\", echo=True)\n","  Xsam, Ysam, clas, sam = out\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/Xsam.pkl', 'wb')\n","  pickle.dump(Xsam, pick_insert)\n","  pick_insert.close()\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/Ysam.pkl', 'wb')\n","  pickle.dump(Ysam, pick_insert)\n","  pick_insert.close()\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/clas.pkl', 'wb')\n","  pickle.dump(clas, pick_insert)\n","  pick_insert.close()\n","\n","  pick_insert = open(f'{HOME}pickled_database/{DSNAME}/sam.pkl', 'wb')\n","  pickle.dump(sam, pick_insert)X_raw[0]\n","  pick_insert.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dss93f4xx_nG","outputId":"93503c6e-f39b-49ad-d187-e0af84fcd51d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["skipping\n"]}]},{"cell_type":"markdown","source":["## Bloque 3: Extracción de características.\n","Librerías necesarias para este bloque:"],"metadata":{"id":"7hIGi3eiFUr0"}},{"cell_type":"code","source":["from IPython.display import clear_output\n","!pip3 install scipy==1.2\n","!pip3 install pybalu==0.2.5\n","clear_output()"],"metadata":{"id":"LNBGqeltm6sZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pybalu.feature_extraction import hog_features, lbp_features"],"metadata":{"id":"W7z6etBgILKp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Formato del nombre para cada característica soportada:\n","  - HoG: \"HOG-NxMxB\"\n","  - LBP: \"LBP-NxM\"\n","\n","Formato de archivo de características guardado en Drive:\n","\n","\"DSNAME_feature1+feature2.pkl\"\n","\n","Ej:\n","\"Bicicletas_LBP-5x5+HOG-7x7x9.pkl\""],"metadata":{"id":"yXaj5rg6lfuZ"}},{"cell_type":"code","source":["class Block3:\n","  def __init__(self, DSNAME, features, load_database=True, load_features=True, save=True):\n","      '''\n","      Extracts the features indicated from the DSNAME dataset.\n","\n","      INPUT:\n","        DSNAME: name of the dataset stored in Drive.\n","        features: array of strings following feature name convention,\n","        load_database: True -> loads pickled images directly.\n","        load_features: True -> tries to load the features from Drive in case they already\n","          been extracted before. If file does not exist, the features are extracted.\n","        save:          True -> saves the extracted features to Drive.\n","\n","      OUTPUT:\n","        Feature matrix is stored in the variable self.X.\n","        Ground truth is stored in the variable self.Ysam\n","      '''\n","      self.DSNAME = DSNAME\n","      self.features = features\n","      self.features_names, self.features_parameters = self.parse_features()\n","      self.save = save\n","\n","      if load_database:\n","        self.Xsam, self.Ysam, self.n_class, self.n_samples = self.import_pickled_dataset()\n","      else:\n","        self.Xsam, self.Ysam, self.n_class, self.n_samples = BuildDataset(f\"{HOME}raw_database/{self.DSNAME}/\", echo=True)\n","      self.n_class += 1\n","\n","      self.identifier = self.DSNAME + \"_\" + \"+\".join(self.features)\n","      if load_features:\n","        self.X = self.load_features()\n","      else:\n","        self.X = self.extract_features()\n","  \n","  def parse_features(self):\n","    features_names = []\n","    features_parameters = dict()\n","\n","    for feature in self.features:\n","      name = feature.split(\"-\")[0]\n","      parameters = feature.split(\"-\")[1]\n","      if name == \"HOG\":\n","        parameters = [int(param) for param in parameters.split(\"x\")]\n","\n","      elif name == \"LBP\":\n","        parameters = [int(param) for param in parameters.split(\"x\")]\n","      features_names.append(name)\n","      features_parameters[name] = parameters\n","    \n","    return features_names, features_parameters\n","    \n","\n","  def import_pickled_dataset(self):\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/Xsam.pkl','rb')\n","    Xsam = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/Ysam.pkl','rb')\n","    Ysam = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/clas.pkl','rb')\n","    n_class = pickle.load(file_read)\n","    file_read.close()\n","\n","    file_read = open(f'{HOME}pickled_database/{self.DSNAME}/sam.pkl','rb')\n","    n_samples = pickle.load(file_read)\n","    file_read.close()\n","    return Xsam, Ysam, n_class, n_samples\n","\n","  def load_features(self):\n","    try:\n","      read_file = open(f'{HOME}pickled_features/{self.DSNAME}/{self.identifier}.pkl', 'rb')\n","      X = pickle.load(read_file)\n","      return X\n","    except:\n","      print(f'Could not load {self.identifier}.pkl')\n","      X = self.extract_features()\n","      return X\n","\n","  def extract_features(self):\n","    total_images = sum(self.n_samples)\n","    feature_matrices = dict() # Dictionary indexed by feature name that saves feature matrix of feature.\n","\n","    # Initialize feature matrices\n","    for feature in self.features_names:\n","      if feature == \"LBP\":\n","        parameters = self.features_parameters[feature]\n","        M = 59*parameters[0]*parameters[1]\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","\n","      elif feature == \"HOG\":\n","        parameters = self.features_parameters[feature]\n","        M = parameters[0]*parameters[1]*parameters[2]\n","        Xfeat = np.zeros((total_images,M))\n","        feature_matrices[feature] = Xfeat\n","    \n","    # Extract each feature from each image.\n","    t = 0\n","    for image in self.Xsam:\n","      for feature in self.features_names:\n","        if feature == \"LBP\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = lbp_features(image, hdiv=parameters[0], vdiv=parameters[1], mapping='nri_uniform')\n","\n","        elif feature == \"HOG\":\n","          parameters = self.features_parameters[feature]\n","          feature_matrices[feature][t,:] = hog_features(image, v_windows=parameters[0], h_windows=parameters[1], n_bins=parameters[2])\n","      t+=1\n","    \n","    # Concatenate features\n","    X = np.concatenate(tuple([feature_matrices[feature] for feature in self.features_names]), axis=1)\n","\n","    if self.save:\n","      try:\n","        pick_insert = open(f'{HOME}pickled_features/{self.DSNAME}/{self.identifier}.pkl', 'wb')\n","        pickle.dump(X, pick_insert)\n","        pick_insert.close()\n","      except:\n","        print(f'Could not save {self.identifier}.pkl')\n","\n","    return X\n","\n","  def __str__(self):\n","    return self.identifier"],"metadata":{"id":"mg2U1ZGol_ZA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ejemplo de uso"],"metadata":{"id":"KnKiJy0pJ2zy"}},{"cell_type":"code","source":["block3 = Block3(\"Espinas\", [\"LBP-6x6\", \"HOG-7x7x9\"], load_database=True, load_features=False, save=True)"],"metadata":{"id":"C4YZjirdqKez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(block3)\n","print(block3.features_names, block3.features_parameters)\n","print(block3.Xsam.shape, block3.Ysam.shape, block3.n_class, block3.n_samples)\n","print(block3.X.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ks4QvpQZuiLR","executionInfo":{"status":"ok","timestamp":1655762843935,"user_tz":240,"elapsed":5,"user":{"displayName":"SEBASTIAN ZABALA","userId":"08365423652520134099"}},"outputId":"8f62a795-64ef-4070-cb8d-2dee330ea559"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Espinas_LBP-5x5+HOG-7x7x9\n","['LBP', 'HOG'] {'LBP': [5, 5], 'HOG': [7, 7, 9]}\n","(640, 100, 100) (640, 1) 2 [320, 320]\n","(640, 1916)\n"]}]},{"cell_type":"code","source":["del block3 # Importante para no llenar la RAM mientras se prueba"],"metadata":{"id":"fqdxzmgu2iDe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bloque 4: selección y transformación de características.\n","- Split Train-Validation\n","- Aplicación secuencial de alguna selección/transformación\n","  - Clean\n","  - MinMax Scaling\n","  - SelectKBest\n","  - SFS\n","  - PCA\n","  - ICA"],"metadata":{"id":"N1flNF6VEERS"}},{"cell_type":"code","source":["from IPython.display import clear_output\n","!pip3 install scipy==1.2\n","!pip3 install pybalu==0.2.5\n","clear_output()"],"metadata":{"id":"Lnx6MMNcm4Th"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from pybalu.feature_selection import clean\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.feature_selection import SelectKBest, chi2\n","from pybalu.feature_selection import sfs\n","from sklearn.decomposition import PCA, FastICA"],"metadata":{"id":"NVNNFHwbKdeh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CleanInterface:\n","\n","  def __init__(self, X):\n","    self.model = clean(X)\n","\n","  def transform(self, X):\n","    return X[:, self.model]\n","\n","class SFSInterface:\n","\n","  def __init__(self, X, y, s):\n","    self.model = sfs(X, y, s, show=False)\n","\n","  def transform(self, X):\n","    return X[:, self.model]"],"metadata":{"id":"wPSGbmLbOndK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Block4:\n","\n","  def __init__(self, X, y, sequence, ratio=0.3):\n","\n","    self.sequence = sequence\n","\n","    self.models = []\n","\n","    self.Xtrain, self.Xval, self.ytrain, self.yval = train_test_split(np.array(X), y, test_size=ratio, random_state=42, stratify=y)\n","\n","    self.Xtrain = self.interative_fit()\n","    self.Xval = self.transform(self.Xval)\n","\n","  def interative_fit(self):\n","\n","    X = self.Xtrain\n","\n","    for seq in self.sequence:\n","\n","      name = seq.split('-')[0]\n","\n","      if name == 'CLEAN':\n","        model = CleanInterface(X)\n","      elif name == 'MINMAX':\n","        model = MinMaxScaler().fit(X)\n","      else: \n","        param = int(seq.split('-')[1])\n","        if name == 'KBEST':\n","          model = SelectKBest(chi2, k=param).fit(X, self.ytrain)\n","        elif name == 'SFS':\n","          model = SFSInterface(X, self.ytrain, param)\n","        elif name == 'PCA':\n","          model = PCA(n_components=param).fit(X)\n","        elif name == 'ICA':\n","          model = FastICA(n_components=param, random_state=0).fit(X, self.ytrain)\n","        else:\n","          model = None\n","          print(f'No existe el modelo {name}')\n","\n","      self.models.append(model)\n","      X = model.transform(X)\n","\n","    return X\n","  \n","  def transform(self, X):\n","\n","    for model in self.models:\n","      X = model.transform(X)\n","    return X"],"metadata":{"id":"qYdP2hn7GEeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DATOS DE PRUEBA\n","!gdown --id 1CA-l9_JjdjG_4kTuavKf8Wm27dt0jyqT\n","clear_output()\n","\n","f = open('data.p', \"rb\")\n","data = pickle.load(f)\n","X = data['train']\n","\n","y = np.array([0 if i < 7000 else 1 for i in range(0, 14000)])\n","\n","X.shape, len(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3N8G2DVGJiA","outputId":"50d51bd1-06bd-460c-88e9-357e3be28d41"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14000, 1844), 14000)"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["block = Block4(np.array(X), y, ['CLEAN', 'MINMAX', 'KBEST-50', 'SFS-10', 'PCA-5', 'ICA-2'])\n","block.Xtrain.shape, block.Xval.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxZu_HFjM7rE","outputId":"128fb986-4f6b-4d8f-ff3f-f5d2638f3856"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(9800, 1844) (4200, 1844) (9800,) (4200,)\n"]},{"output_type":"execute_result","data":{"text/plain":["((9800, 2), (4200, 2))"]},"metadata":{},"execution_count":72}]}],"metadata":{"interpreter":{"hash":"6189f557f5bc4df41e92f7a00b3721572a07e07697aeafd1cb02e690e443f281"},"kernelspec":{"display_name":"Python 3.9.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"orig_nbformat":4,"colab":{"name":"base.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}